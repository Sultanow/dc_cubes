{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "        \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "core_name = \"dc_cubes_historic\"\n",
    "predictionColumn = \"cpuusage_ps\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteSolrCore(core_name, deleteEverything=True):  # l√∂scht immer den ganzen core\n",
    "    url = \"http://localhost:8983/solr/admin/cores?action=UNLOAD&core=\"+core_name\n",
    "\n",
    "    if (deleteEverything):\n",
    "        url += \"&deleteInstanceDir=true\"\n",
    "    requests.get(url)\n",
    "    print(core_name, \" core deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushData(row):\n",
    "    global core_name\n",
    "    global counter\n",
    "    global allMetrics\n",
    "    global predictionColumn\n",
    "    # defining the api-endpoint\n",
    "    url = \"http://localhost:8983/solr/\"+core_name+\"/update/json/docs\"\n",
    "    # data to be sent to api\n",
    "    data = {\n",
    "                \"timestamp\": str(row[\"timestamp\"]),\n",
    "                \"cluster\": row[\"cluster\"],\n",
    "                \"dc\": row[\"dc\"],\n",
    "                \"perm\": -1, #row[\"perm\"],\n",
    "                \"instanz\": row[\"instanz\"],\n",
    "                \"verfahren\": \"-1\",# row[\"verfahren\"],\n",
    "                \"service\": \"-1\" ,#row[\"verfahren\"],\n",
    "                \"response\": -1 ,#row[\"response\"],\n",
    "                \"count\": row[predictionColumn],\n",
    "                \"minv\":  -1,#row[\"minv\"],\n",
    "                \"maxv\":  -1,#row[\"maxv\"],\n",
    "                \"avg\": -1, #row[\"avg\"],\n",
    "                \"var\": -1 ,#row[\"var\"],\n",
    "                \"dev_upp\": -1, #row[\"dev_upp\"],\n",
    "                \"dev_low\": -1, #row[\"dev_low\"],\n",
    "                \"perc90\": -1 ,#row[\"perc90\"],\n",
    "                \"perc95\": -1 ,#row[\"perc95\"],\n",
    "                \"perc99\": -1 ,#row[\"perc99.9\"],\n",
    "                \"sum\": -1, #row[\"sum\"],\n",
    "                \"sum_of_squares\": -1, #row[\"sum_of_squares\"],\n",
    "                \"server\": \"PBZ0%dE00_PERM02_S0%s_OSB\" % (row[\"cluster\"], row[\"instanz\"]), #row[\"server\"]\n",
    "            }\n",
    "    \n",
    "    for metric in allMetrics:\n",
    "        data[metric] = row[metric]\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    session.get(url)\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    # sending post request\n",
    "    session.post(url=url, data=json.dumps(data), headers=headers)\n",
    "    counter += 1\n",
    "    if (counter % 1000 == 0):\n",
    "        print(\"Commiting... counter:\", counter)\n",
    "        requests.get(\"http://localhost:8983/solr/\"+core_name+\"/update?commit=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSolrCore(core_name):\n",
    "    url = \"http://localhost:8983/solr/admin/cores?action=CREATE&name=\" + \\\n",
    "        core_name+\"&configSet=_default\"\n",
    "    requests.post(url=url)\n",
    "    print(core_name, \" created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSchema(core_name, allMetrics):\n",
    "    url = \"http://localhost:8983/solr/\"+core_name+\"/schema\"\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    rowsDict = {\n",
    "        \"timestamp\": \"pdate\", \"host\": \"string\", \"cluster\": \"pint\", \"dc\": \"pint\", \"perm\": \"pint\", \"instanz\": \"string\", \"verfahren\": \"string\",\n",
    "        \"service\": \"string\", \"response\": \"pint\", \"count\": \"pfloat\", \"minv\": \"pint\", \"maxv\": \"pint\", \"avg\": \"pfloat\", \"var\": \"pfloat\",\n",
    "        \"dev_upp\": \"pfloat\", \"dev_low\": \"pfloat\", \"perc90\": \"pfloat\", \"perc95\": \"pfloat\", \"perc99\": \"pfloat\", \"sum\": \"pint\",\n",
    "        \"sum_of_squares\": \"pint\", \"server\": \"string\"}\n",
    "\n",
    "    for name in rowsDict:\n",
    "        data = {\n",
    "            \"add-field\": {\"stored\": \"true\", \"docValues\": \"true\", \"indexed\": \"false\", \"multiValued\": \"false\", \"name\": name, \"type\": rowsDict[name]}\n",
    "        }\n",
    "        requests.post(url=url, data=json.dumps(data), headers=headers)\n",
    "        \n",
    "    for metric in allMetrics:\n",
    "        if metric not in rowsDict:\n",
    "            data = {\n",
    "                \"add-field\": {\"stored\": \"true\", \"docValues\": \"true\", \"indexed\": \"false\", \"multiValued\": \"false\", \"name\": metric, \"type\": \"pfloat\"}\n",
    "            }\n",
    "            requests.post(url=url, data=json.dumps(data), headers=headers)\n",
    "    \n",
    "    \n",
    "    print(core_name, \" schema inited\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteCoreDocuments(core_name):\n",
    "    url = \"http://localhost:8983/solr/\"+core_name + \\\n",
    "        \"/update?commitWithin=1000&overwrite=true&wt=json\"\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    data = {'delete': {'query': '*:*'}}\n",
    "    requests.post(url=url, data=json.dumps(data), headers=headers)\n",
    "    print(\"deleted old documents from \"+core_name+\" core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(core_name):\n",
    "    # from 1 Month, there are about 90k entries. 2 clusters * 2 dcs * 8 instances * (4 weeks * 7 days * 24 hours * 4 (15min intervall))\n",
    "    url = 'http://localhost:8983/solr/'+core_name+'/select?q=*:*&sort=timestamp%20asc&rows=100000'\n",
    "    response = requests.get(url).json()['response']\n",
    "    response\n",
    "    return response['docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get existing solr cores\n",
    "url = \"http://localhost:8983/solr/admin/cores?action=STATUS\"\n",
    "response = requests.get(url).json()\n",
    "activeCores = response['status'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc_cubes_historic already exists\n",
      "deleted old documents from dc_cubes_historic core\n"
     ]
    }
   ],
   "source": [
    "if core_name in activeCores:\n",
    "    print(core_name + \" already exists\")\n",
    "    # delete old data/predictions\n",
    "    deleteCoreDocuments(core_name)\n",
    "# else forecast core doesn't exist\n",
    "else:\n",
    "    print(core_name + \" doesn't exist\")\n",
    "    # create an new forecast solr core\n",
    "    createSolrCore(core_name)\n",
    "    # init schema\n",
    "    initSchema(core_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from pickle file\n",
    "with open(\"./data_pblm1.pkl\", \"rb\") as pickleFile:\n",
    "    pblm1 = pickle.load(pickleFile)\n",
    "pblm1 = pblm1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from pickle file\n",
    "with open(\"./data_pblm2.pkl\", \"rb\") as pickleFile:\n",
    "    pblm2 = pickle.load(pickleFile)\n",
    "pblm2 = pblm2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-01-23 09:30:00')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pblm2.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing 2020-01-23 09:30:00\n",
    "copyRow = pblm2[pblm2.timestamp == pblm2.timestamp.max()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>transactions_ps</th>\n",
       "      <th>opt_sga_size</th>\n",
       "      <th>sga_total</th>\n",
       "      <th>fixed_sga</th>\n",
       "      <th>java_pool</th>\n",
       "      <th>shared_pool</th>\n",
       "      <th>buffer_cache</th>\n",
       "      <th>pga_total</th>\n",
       "      <th>...</th>\n",
       "      <th>pxdwngrd25_ps</th>\n",
       "      <th>dictionarymiss_pct</th>\n",
       "      <th>cpuusage_ps</th>\n",
       "      <th>pxdwngrd50_ps</th>\n",
       "      <th>avg_tot_cpu_usage_ps</th>\n",
       "      <th>max_tot_cpu_usage_ps</th>\n",
       "      <th>dayOfWeek</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>2787</td>\n",
       "      <td>2020-01-23 09:45:00</td>\n",
       "      <td>0.002</td>\n",
       "      <td>10240</td>\n",
       "      <td>20479.998</td>\n",
       "      <td>21.732</td>\n",
       "      <td>320</td>\n",
       "      <td>4928.0</td>\n",
       "      <td>14336</td>\n",
       "      <td>1213.082</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0</td>\n",
       "      <td>3.005974</td>\n",
       "      <td>6.708903</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index           timestamp  transactions_ps  opt_sga_size  sga_total  \\\n",
       "2787   2787 2020-01-23 09:45:00            0.002         10240  20479.998   \n",
       "\n",
       "      fixed_sga  java_pool  shared_pool  buffer_cache  pga_total  ...  \\\n",
       "2787     21.732        320       4928.0         14336   1213.082  ...   \n",
       "\n",
       "      pxdwngrd25_ps  dictionarymiss_pct  cpuusage_ps  pxdwngrd50_ps  \\\n",
       "2787              0                 0.0        0.449              0   \n",
       "\n",
       "      avg_tot_cpu_usage_ps  max_tot_cpu_usage_ps  dayOfWeek  isWeekend  hour  \\\n",
       "2787              3.005974              6.708903          3          0     9   \n",
       "\n",
       "      minute  \n",
       "2787      30  \n",
       "\n",
       "[1 rows x 177 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copyRow.timestamp = pd.Timestamp(\"2020-01-23 09:45:00\")\n",
    "copyRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm2 = pblm2.append(copyRow, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "allMetrics = pblm2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "allMetrics = allMetrics.drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm2[allMetrics] = pblm2[allMetrics].apply(pd.to_numeric, errors=\"coerce\") # cast all to numeric because values arent getting pushed correctly to solr otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm1[allMetrics] = pblm1[allMetrics].apply(pd.to_numeric, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc_cubes_historic  schema inited\n"
     ]
    }
   ],
   "source": [
    "initSchema(core_name, allMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two dc's: 0 and 1\n",
    "# dc 0 has clusters 6 and 8\n",
    "# dc 1 has clusters 5 and 7\n",
    "# Every Cluster has all 8 instances\n",
    "\n",
    "def pushDataForAllInstances(df):\n",
    "    instances = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\" ]\n",
    "    for instanz in instances:\n",
    "        df[\"instanz\"] = instanz\n",
    "        df.apply(pushData, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm2[\"dc\"] = 0 # this will be pblm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm2[\"cluster\"] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commiting... counter: 1000\n",
      "Commiting... counter: 2000\n",
      "Commiting... counter: 3000\n",
      "Commiting... counter: 4000\n",
      "Commiting... counter: 5000\n",
      "Commiting... counter: 6000\n",
      "Commiting... counter: 7000\n",
      "Commiting... counter: 8000\n",
      "Commiting... counter: 9000\n",
      "Commiting... counter: 10000\n",
      "Commiting... counter: 11000\n",
      "Commiting... counter: 12000\n",
      "Commiting... counter: 13000\n",
      "Commiting... counter: 14000\n",
      "Commiting... counter: 15000\n",
      "Commiting... counter: 16000\n",
      "Commiting... counter: 17000\n",
      "Commiting... counter: 18000\n",
      "Commiting... counter: 19000\n",
      "Commiting... counter: 20000\n",
      "Commiting... counter: 21000\n",
      "Commiting... counter: 22000\n"
     ]
    }
   ],
   "source": [
    "pushDataForAllInstances(pblm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm2[\"cluster\"] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commiting... counter: 23000\n",
      "Commiting... counter: 24000\n",
      "Commiting... counter: 25000\n",
      "Commiting... counter: 26000\n",
      "Commiting... counter: 27000\n",
      "Commiting... counter: 28000\n",
      "Commiting... counter: 29000\n",
      "Commiting... counter: 30000\n",
      "Commiting... counter: 31000\n",
      "Commiting... counter: 32000\n",
      "Commiting... counter: 33000\n",
      "Commiting... counter: 34000\n",
      "Commiting... counter: 35000\n",
      "Commiting... counter: 36000\n",
      "Commiting... counter: 37000\n",
      "Commiting... counter: 38000\n",
      "Commiting... counter: 39000\n",
      "Commiting... counter: 40000\n",
      "Commiting... counter: 41000\n",
      "Commiting... counter: 42000\n",
      "Commiting... counter: 43000\n",
      "Commiting... counter: 44000\n"
     ]
    }
   ],
   "source": [
    "pushDataForAllInstances(pblm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm1[\"dc\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm1[\"cluster\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commiting... counter: 45000\n",
      "Commiting... counter: 46000\n",
      "Commiting... counter: 47000\n",
      "Commiting... counter: 48000\n",
      "Commiting... counter: 49000\n",
      "Commiting... counter: 50000\n",
      "Commiting... counter: 51000\n",
      "Commiting... counter: 52000\n",
      "Commiting... counter: 53000\n",
      "Commiting... counter: 54000\n",
      "Commiting... counter: 55000\n",
      "Commiting... counter: 56000\n",
      "Commiting... counter: 57000\n",
      "Commiting... counter: 58000\n",
      "Commiting... counter: 59000\n",
      "Commiting... counter: 60000\n",
      "Commiting... counter: 61000\n",
      "Commiting... counter: 62000\n",
      "Commiting... counter: 63000\n",
      "Commiting... counter: 64000\n",
      "Commiting... counter: 65000\n",
      "Commiting... counter: 66000\n"
     ]
    }
   ],
   "source": [
    "pushDataForAllInstances(pblm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pblm1[\"cluster\"] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commiting... counter: 67000\n",
      "Commiting... counter: 68000\n",
      "Commiting... counter: 69000\n",
      "Commiting... counter: 70000\n",
      "Commiting... counter: 71000\n",
      "Commiting... counter: 72000\n",
      "Commiting... counter: 73000\n",
      "Commiting... counter: 74000\n",
      "Commiting... counter: 75000\n",
      "Commiting... counter: 76000\n",
      "Commiting... counter: 77000\n",
      "Commiting... counter: 78000\n",
      "Commiting... counter: 79000\n",
      "Commiting... counter: 80000\n",
      "Commiting... counter: 81000\n",
      "Commiting... counter: 82000\n",
      "Commiting... counter: 83000\n",
      "Commiting... counter: 84000\n",
      "Commiting... counter: 85000\n",
      "Commiting... counter: 86000\n",
      "Commiting... counter: 87000\n",
      "Commiting... counter: 88000\n",
      "Commiting... counter: 89000\n"
     ]
    }
   ],
   "source": [
    "pushDataForAllInstances(pblm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:8983/solr/\"+core_name+\"/update?commit=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*______________________________________*#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_core_name = \"dc_cubes_forecast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicCoreName = \"dc_cubes_historic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame.from_dict(getData(historicCoreName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "allColumns = hist_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc_cubes_forecast doesn't exist\n",
      "dc_cubes_forecast  created\n",
      "dc_cubes_forecast  schema inited\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:8983/solr/admin/cores?action=STATUS\"\n",
    "response = requests.get(url).json()\n",
    "activeCores = response['status'].keys()\n",
    "\n",
    "# if forecast core exists\n",
    "if forecast_core_name in activeCores:\n",
    "    print(forecast_core_name + \" already exists\")\n",
    "    deleteSolrCore(forecast_core_name)\n",
    "    # delete old data/predictions\n",
    "    createSolrCore(forecast_core_name)\n",
    "    # init schema\n",
    "    initSchema(forecast_core_name, allColumns)\n",
    "    #deleteCoreDocuments(forecast_core_name)\n",
    "# else forecast core doesn't exist\n",
    "else:\n",
    "    print(forecast_core_name + \" doesn't exist\")\n",
    "    # create an new forecast solr core\n",
    "    createSolrCore(forecast_core_name)\n",
    "    # init schema\n",
    "    initSchema(forecast_core_name, allColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the data of each cube from the whole df in its own dataframe and takes only the last -history_steps\n",
    "def splitInCubesFrames(df):\n",
    "    unique_server_names = df.server.unique()\n",
    "    splitted_frames = []\n",
    "    for name in unique_server_names:\n",
    "        new_df = df[df['server'] == name].copy()\n",
    "        splitted_frames.append(new_df)\n",
    "    return splitted_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measureInterval = 15 #min\n",
    "hoursToPredict = 24 * 4\n",
    "pred_horizon = int((60//measureInterval) * hoursToPredict) #(4*hours), timestep = 15min\n",
    "days_history = 1\n",
    "hours_history = int(24 * days_history)\n",
    "n_history = int((60//measureInterval)*hours_history)\n",
    "\n",
    "history_steps = n_history\n",
    "forecast_steps = pred_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_timestamp = hist_df.timestamp.max()\n",
    "hist_df[timestamp] = pd.to_datetime(hist_df.timestamp)\n",
    "# generate features/columns from the timestamp\n",
    "hist_df[\"dayOfWeek\"] = hist_df[timestamp].map(lambda x: x.dayofweek)\n",
    "hist_df[\"isWeekend\"] = hist_df.dayOfWeek.map(lambda x: 0 if (x < 5) else 1) # saturday.dayofweek = 5, monday=0\n",
    "#hist_df[\"weekofyear\"] = hist_df[timestamp].map(lambda x: x.weekofyear)\n",
    "hist_df[\"hour\"] = hist_df[timestamp].map(lambda x: x.hour)\n",
    "hist_df[\"minute\"]= hist_df[timestamp].map(lambda x: x.minute)\n",
    "hist_df = hist_df.set_index('timestamp')\n",
    "\n",
    "hist_df.index = pd.to_datetime(hist_df.index).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# split cubes in own frames\n",
    "cubes_frames = splitInCubesFrames(hist_df)\n",
    "\n",
    "# load the trained model\n",
    "modelDc0 = load_model('final_cnn_pblm2.h5')\n",
    "modelDc1 = load_model('final_cnn_pblm1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredictionFrame(modelDc0, modelDc1, cubes_frames, last_timestamp, predictionColumn = \"cpuusage_ps\"):\n",
    "    prediction_frames = []\n",
    "    for cube in cubes_frames:\n",
    "        # transform pre prediction input\n",
    "\n",
    "        # extracting information from the current server\n",
    "        last_timestamp = cube.index[-1]\n",
    "        server_name = cube['server'].iloc[0]\n",
    "        cluster = cube['cluster'].iloc[0]\n",
    "        dc = cube['dc'].iloc[0]\n",
    "        perm = cube['perm'].iloc[0]\n",
    "        instanz = cube['instanz'].iloc[0]\n",
    "        verfahren = cube['verfahren'].iloc[0]\n",
    "        service = cube['service'].iloc[0]\n",
    "\n",
    "        print(\"DATACENTER: \", dc)\n",
    "        if(int(dc) == 0):\n",
    "            model = modelDc0\n",
    "        else:\n",
    "            model = modelDc1\n",
    "            \n",
    "        dropCols = [\"id\", \"cluster\",\n",
    "                \"_version_\",\n",
    "                \"dc\",\n",
    "                \"perm\",\n",
    "                \"instanz\",\n",
    "                \"verfahren\",\n",
    "                \"service\",\n",
    "                \"response\",\n",
    "                \"minv\",\n",
    "                \"maxv\", \n",
    "                \"avg\",\n",
    "                \"var\",\n",
    "                \"dev_upp\",\n",
    "                \"dev_low\",\n",
    "                \"perc90\",\n",
    "                \"perc95\",\n",
    "                \"perc99\",\n",
    "                \"sum\",\n",
    "                \"sum_of_squares\",\n",
    "                \"server\"]\n",
    "        # Converting the index as date\n",
    "#         pdb.set_trace()\n",
    "        cube.index = pd.to_datetime(cube.index).sort_values()\n",
    "        \n",
    "\n",
    "\n",
    "        cube = cube.drop(dropCols,axis=1)\n",
    "        dataset = cube\n",
    "#         if dc == 0:\n",
    "#             allCols = cube.columns\n",
    "#             cube[allCols] = cube[allCols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "#             dataset = cube.dropna(axis=1, how=\"all\")\n",
    "#             dataset[\"dummy1\"] = 1\n",
    "#             dataset[\"dummy2\"] = 0\n",
    "        y = dataset[predictionColumn].copy() # prediction column was pushed to count\n",
    "        x = dataset.drop(columns=[predictionColumn, \"count\"])\n",
    "#         print(x.shape)\n",
    "        # Standardise\n",
    "        allMetrics =  dataset.columns.tolist()\n",
    "        print(\"X SHAPE\", x.shape)\n",
    "\n",
    "        # standardise\n",
    "        scalerX = StandardScaler()\n",
    "        scalerX.fit(x)\n",
    "        x = scalerX.transform(x)\n",
    "        scalerY = StandardScaler()\n",
    "       # .reshape(-1, 1) # needed for standardScaler\n",
    "        scalerY.fit(y.values.reshape(-1,1))\n",
    "        y = scalerY.transform(y.values.reshape(-1,1))\n",
    "        \n",
    "        #PCA\n",
    "#         pcaTransformer = PCA(n_components=63) # keep 95% variance\n",
    "#         pcaTransformer.fit(x)\n",
    "#         x = pcaTransformer.transform(x)\n",
    "\n",
    "        #transformed_df = pd.DataFrame().from_records(x)\n",
    "        #transformed_df[predictionColumn] = y\n",
    "        numberOfFeatures = x.shape[1]\n",
    "        # predict\n",
    "        pred_input = x[x.shape[0]-history_steps:]\n",
    "#         print(\"***mean, std\", x.mean(), x.std())\n",
    "#         print(\"************\\n\", pred_input)\n",
    "#         pdb.set_trace()\n",
    "        pred_input = pred_input.reshape(\n",
    "            (1, history_steps, numberOfFeatures)) # alternative (1, pred_input.shape[0],pred_input.shape[1])\n",
    "        prediction = model.predict(pred_input)\n",
    "#         print(\"prediction: \", prediction, prediction.shape)\n",
    "        #prediction = np.hstack((prediction, np.zeros((prediction.shape[0], numberOfFeatures-1), dtype=prediction.dtype)))\n",
    "        \n",
    "    #    print(\"pre inverse scaling\", prediction)\n",
    "        prediction = scalerY.inverse_transform(prediction)\n",
    "#        print(\"post inverse scaling\", prediction)\n",
    "        # transform pre solr\n",
    "#         prediction = prediction.reshape((forecast_steps, 1))\n",
    "#         prediction = np.hstack((prediction, np.zeros(\n",
    "#             (prediction.shape[0], 3), dtype=prediction.dtype)))\n",
    "#         prediction = prediction = scaler.inverse_transform(prediction)\n",
    "#         prediction = prediction[:, [0]]\n",
    "#         int_prediction = prediction.astype(int, copy=True)\n",
    "#         prediction = int_prediction\n",
    "\n",
    "        prediction = prediction.reshape(prediction.shape[1])\n",
    "        # make the dataframe\n",
    "        next_timestamps = pd.date_range(\n",
    "            start=last_timestamp, periods=forecast_steps+1, freq='15min',  closed='right')\n",
    "        # create the prediction dataframe for the current server\n",
    "        d = {'timestamp': next_timestamps, 'cluster': cluster, 'dc': dc,\n",
    "             'perm': perm, 'instanz': instanz,  'verfahren': verfahren, 'service': service, 'response': 200}\n",
    "        pred_df = pd.DataFrame(data=d)\n",
    "        for metric in allMetrics:\n",
    "            pred_df[metric] = -1\n",
    "#         print(\"Length:\", len(pred_df), \"::::::\", prediction.shape)\n",
    "        pred_df['count'] = prediction\n",
    "        pred_df[predictionColumn] = prediction\n",
    "        pred_df['minv'] = 0\n",
    "        pred_df['maxv'] = 0\n",
    "        pred_df['avg'] = 0\n",
    "        pred_df['var'] = 0\n",
    "        pred_df['dev_upp'] = 0\n",
    "        pred_df['dev_low'] = 0\n",
    "        pred_df['perc90'] = 0\n",
    "        pred_df['perc95'] = 0\n",
    "        pred_df['perc99.9'] = 0\n",
    "        pred_df['sum'] = 0\n",
    "        pred_df['sum_of_squares'] = 0\n",
    "        pred_df['server'] = server_name\n",
    "\n",
    "        pred_df['timestamp'] = pred_df['timestamp'].dt.strftime(\n",
    "            '%Y-%m-%dT%H:%M:00Z')\n",
    "        prediction_frames.append(pred_df)\n",
    "    print(\"Made predictions\")\n",
    "    return pd.concat(prediction_frames, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  0\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "DATACENTER:  1\n",
      "X SHAPE (2789, 176)\n",
      "Made predictions\n"
     ]
    }
   ],
   "source": [
    "prediction_df = makePredictionFrame(modelDc0, modelDc1, cubes_frames, last_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushForecastData(row):\n",
    "    global forecast_core_name\n",
    "    global counter\n",
    "    global allColumns\n",
    "    # defining the api-endpoint\n",
    "    url = \"http://localhost:8983/solr/\"+forecast_core_name+\"/update/json/docs\"\n",
    "    # data to be sent to api\n",
    "    data = {\n",
    "                \"timestamp\": row[\"timestamp\"],\n",
    "                \"cluster\": row[\"cluster\"],\n",
    "                \"dc\": row[\"dc\"],\n",
    "                \"perm\": row[\"perm\"],\n",
    "                \"instanz\": row[\"instanz\"],\n",
    "                \"verfahren\": row[\"verfahren\"],\n",
    "                \"service\": row[\"verfahren\"],\n",
    "                \"response\": row[\"response\"],\n",
    "                \"count\": row[\"count\"],\n",
    "                \"minv\": row[\"minv\"],\n",
    "                \"maxv\": row[\"maxv\"],\n",
    "                \"avg\": row[\"avg\"],\n",
    "                \"var\": row[\"var\"],\n",
    "                \"dev_upp\": row[\"dev_upp\"],\n",
    "                \"dev_low\": row[\"dev_low\"],\n",
    "                \"perc90\": row[\"perc90\"],\n",
    "                \"perc95\": row[\"perc95\"],\n",
    "                \"perc99\": row[\"perc99.9\"],\n",
    "                \"sum\": row[\"sum\"],\n",
    "                \"sum_of_squares\": row[\"sum_of_squares\"],\n",
    "                \"server\": row[\"server\"],\n",
    "                \"cpuusage_ps\": row[\"cpuusage_ps\"]\n",
    "            }\n",
    "    \n",
    "    for col in allColumns:\n",
    "        if col not in data:\n",
    "            data[col] = -1\n",
    "    data.pop(\"id\", None)\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    session.get(url)\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    # sending post request\n",
    "    session.post(url=url, data=json.dumps(data), headers=headers)\n",
    "    counter += 1\n",
    "    if (counter % 1000 == 0):\n",
    "        print(\"Commiting... counter:\", counter)\n",
    "        requests.get(\"http://localhost:8983/solr/\"+forecast_core_name+\"/update?commit=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cluster</th>\n",
       "      <th>dc</th>\n",
       "      <th>perm</th>\n",
       "      <th>instanz</th>\n",
       "      <th>verfahren</th>\n",
       "      <th>service</th>\n",
       "      <th>response</th>\n",
       "      <th>level_0</th>\n",
       "      <th>count</th>\n",
       "      <th>...</th>\n",
       "      <th>avg</th>\n",
       "      <th>var</th>\n",
       "      <th>dev_upp</th>\n",
       "      <th>dev_low</th>\n",
       "      <th>perc90</th>\n",
       "      <th>perc95</th>\n",
       "      <th>perc99.9</th>\n",
       "      <th>sum</th>\n",
       "      <th>sum_of_squares</th>\n",
       "      <th>server</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-23T10:00:00Z</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>200</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.439919</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PBZ08E00_PERM02_S02_OSB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-23T10:15:00Z</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>200</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.080806</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PBZ08E00_PERM02_S02_OSB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-23T10:30:00Z</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>200</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.854486</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PBZ08E00_PERM02_S02_OSB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-23T10:45:00Z</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>200</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.865069</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PBZ08E00_PERM02_S02_OSB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-23T11:00:00Z</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>200</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.048858</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PBZ08E00_PERM02_S02_OSB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp  cluster  dc  perm instanz verfahren service  \\\n",
       "0  2020-01-23T10:00:00Z        8   0    -1       2        -1      -1   \n",
       "1  2020-01-23T10:15:00Z        8   0    -1       2        -1      -1   \n",
       "2  2020-01-23T10:30:00Z        8   0    -1       2        -1      -1   \n",
       "3  2020-01-23T10:45:00Z        8   0    -1       2        -1      -1   \n",
       "4  2020-01-23T11:00:00Z        8   0    -1       2        -1      -1   \n",
       "\n",
       "   response  level_0     count  ...  avg  var  dev_upp  dev_low  perc90  \\\n",
       "0       200       -1  1.439919  ...    0    0        0        0       0   \n",
       "1       200       -1  1.080806  ...    0    0        0        0       0   \n",
       "2       200       -1  0.854486  ...    0    0        0        0       0   \n",
       "3       200       -1  0.865069  ...    0    0        0        0       0   \n",
       "4       200       -1  1.048858  ...    0    0        0        0       0   \n",
       "\n",
       "   perc95  perc99.9  sum  sum_of_squares                   server  \n",
       "0       0         0    0               0  PBZ08E00_PERM02_S02_OSB  \n",
       "1       0         0    0               0  PBZ08E00_PERM02_S02_OSB  \n",
       "2       0         0    0               0  PBZ08E00_PERM02_S02_OSB  \n",
       "3       0         0    0               0  PBZ08E00_PERM02_S02_OSB  \n",
       "4       0         0    0               0  PBZ08E00_PERM02_S02_OSB  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commiting... counter: 90000\n",
      "Commiting... counter: 91000\n",
      "Commiting... counter: 92000\n",
      "Commiting... counter: 93000\n",
      "Commiting... counter: 94000\n",
      "Commiting... counter: 95000\n",
      "Commiting... counter: 96000\n",
      "Commiting... counter: 97000\n",
      "Commiting... counter: 98000\n",
      "Commiting... counter: 99000\n",
      "Commiting... counter: 100000\n",
      "Commiting... counter: 101000\n",
      "Last Commit...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [500]>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.apply(pushForecastData, axis=1)\n",
    "print(\"Last Commit...\")\n",
    "requests.get(\"http://localhost:8983/solr/\"+forecast_core_name+\"/update?commit=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_core_name = \"dc_cubes_merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeTwoCores(merged_core_name, core1, core2):\n",
    "    url = \"http://localhost:8983/solr/admin/cores?action=mergeindexes&core=\" + merged_core_name + \"&srcCore=\" + core1 + \"&srcCore=\" + core2\n",
    "    requests.get(url=url)\n",
    "    # Commit to materialize changes\n",
    "    requests.get(\"http://localhost:8983/solr/\"+merged_core_name+\"/update?commit=true\")\n",
    "    print(core1 + \" and \" +  core2 + \" have been merged to \" + merged_core_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc_cubes_merged already exists\n",
      "deleted old documents from dc_cubes_merged core\n",
      "dc_cubes_historic and dc_cubes_forecast have been merged to dc_cubes_merged\n"
     ]
    }
   ],
   "source": [
    "# if merged core exists\n",
    "if merged_core_name in activeCores:\n",
    "    print(merged_core_name + \" already exists\")\n",
    "    # delete old data/predictions\n",
    "    deleteCoreDocuments(merged_core_name)\n",
    "# else forecast core doesn't exist\n",
    "else:\n",
    "    print(merged_core_name + \" doesn't exist\")\n",
    "    # create an new forecast solr core\n",
    "    createSolrCore(merged_core_name)\n",
    "    # init schema\n",
    "    initSchema(merged_core_name, allColumns)\n",
    "\n",
    "# merged historic and forecast core\n",
    "mergeTwoCores(merged_core_name, historicCoreName, forecast_core_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
