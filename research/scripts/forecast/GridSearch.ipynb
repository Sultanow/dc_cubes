{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "from numpy import array\n",
    "from datetime import timedelta  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from fbprophet import Prophet\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX, SARIMAXResults\n",
    "from pmdarima import auto_arima\n",
    "from datetime import timedelta \n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = \"timestamp\"\n",
    "predictionColumn = \"cpuusage_ps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "# https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def split_train_test(x, y, pred_horizon):\n",
    "    x_train = x[:-1]\n",
    "    x_test = x[-1:]\n",
    "    y_train = y[:-1]\n",
    "    y_test = y[-1:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def split_train_validation(x, y, split):\n",
    "    x_train = x[:-split]\n",
    "    x_val = x[-split:]\n",
    "    y_train = y[:-split]\n",
    "    y_val = y[-split:]\n",
    "    \n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(numberOfHiddenLayers=3, numberOfCells=50, activation=\"relu\", n_history=1, pred_horizon=1, numberOfFeatures=1):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(numberOfCells, activation=activation, return_sequences=True, input_shape=(n_history, numberOfFeatures)))\n",
    "    for i in range(numberOfHiddenLayers-1):\n",
    "        model.add(LSTM(numberOfCells, activation=activation, return_sequences=True))\n",
    "        \n",
    "    model.add(LSTM(numberOfCells, activation=activation, return_sequences=False))    \n",
    "    model.add(Dense(pred_horizon, activation=activation))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def create_gru(numberOfHiddenLayers=3, numberOfCells=50, activation=\"relu\", n_history=1, pred_horizon=1, numberOfFeatures=1):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(numberOfCells, activation=activation, return_sequences=True, input_shape=(n_history, numberOfFeatures)))\n",
    "    for i in range(numberOfHiddenLayers-1):\n",
    "        model.add(GRU(numberOfCells, activation=activation, return_sequences=True))\n",
    "        \n",
    "    model.add(GRU(numberOfCells, activation=activation, return_sequences=False))    \n",
    "    model.add(Dense(pred_horizon, activation=activation))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def create_mlp(numberOfHiddenLayers=3, numberOfNeurons=50, activation=\"relu\", n_history=1, pred_horizon=1, n_input=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(numberOfNeurons, activation=activation, input_dim=n_input))\n",
    "    \n",
    "    for i in range(numberOfHiddenLayers):\n",
    "        model.add(Dense(numberOfNeurons, activation=activation))\n",
    "        \n",
    "    model.add(Dense(pred_horizon))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def create_cnn(numberOfHiddenConvPoolLayers = 1, filters=[75], kernelSize = 3, poolSize = 2, activation = \"relu\",\n",
    "               numberOfLstmLayers = 1, numberOfLstmCells=50, lstmActivation = \"tanh\", \n",
    "               n_history=1, pred_horizon=1, numberOfFeatures = 1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters[0], kernel_size=kernelSize, activation=activation, input_shape=(n_history, numberOfFeatures)))\n",
    "    #model.add(MaxPooling1D(pool_size=poolSize))\n",
    "    \n",
    "    for i in range(numberOfHiddenConvPoolLayers - 1):\n",
    "        model.add(Conv1D(filters=filters[i], kernel_size=kernelSize, activation=activation))\n",
    "        #model.add(MaxPooling1D(pool_size=poolSize))\n",
    "    \n",
    "    for j in range(numberOfLstmLayers-1):\n",
    "        model.add(LSTM(numberOfLstmCells, activation=lstmActivation,return_sequences=True))\n",
    "    if (numberOfLstmLayers > 0):\n",
    "        model.add(LSTM(numberOfLstmCells, activation=lstmActivation))\n",
    "        \n",
    "    model.add(Dense(pred_horizon))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_data_preprocessing(df, n_history, pred_horizon, performPCA):\n",
    "    if not df.index.name == timestamp:\n",
    "        dataset = df.set_index(timestamp)\n",
    "    else:\n",
    "        dataset = df\n",
    "    \n",
    "    y = dataset[predictionColumn].copy()\n",
    "    x = dataset.drop(columns=predictionColumn)\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(x)\n",
    "    x = scalerX.transform(x)\n",
    "    scalerY = StandardScaler()\n",
    "   # .reshape(-1, 1) # needed for standardScaler\n",
    "    scalerY.fit(y.values.reshape(-1,1))\n",
    "    y = scalerY.transform(y.values.reshape(-1,1))\n",
    "    \n",
    "    if(performPCA):\n",
    "        pcaTransformer = PCA(0.95) # keep 95% variance\n",
    "        pcaTransformer.fit(x)\n",
    "        x = pcaTransformer.transform(x)\n",
    "        print(''' *** PCA Result***\\n Started with %d features, reduced to %d features''' \n",
    "              % (len(df.columns)-1, pcaTransformer.n_components_))\n",
    "        \n",
    "    transformed_df = pd.DataFrame().from_records(x)\n",
    "    transformed_df[predictionColumn] = y\n",
    "    \n",
    "    transformed_df.reset_index(inplace=True)\n",
    "    x, y = split_sequences(transformed_df.values, n_steps_in=n_history, n_steps_out=pred_horizon)\n",
    "    return x, y, scalerX, scalerY\n",
    "\n",
    "def train_lstm(df, n_history, pred_horizon, paramDict, performPCA=False):\n",
    "    # data split / preprocessing\n",
    "    preprocessingResultFile = \"grid_lstm_preprocessingResult.pkl\"\n",
    "    if not os.path.isfile(preprocessingResultFile):\n",
    "        print(\"Preprocessing data...\")\n",
    "        # data split / preprocessing\n",
    "        x, y, scalerX, scalerY = lstm_data_preprocessing(df, n_history=n_history, pred_horizon=pred_horizon, performPCA=performPCA)\n",
    "        print(\"SHAPES: \", x.shape, y.shape)\n",
    "        numberOfFeatures = x.shape[2]\n",
    "        x_train, x_test, y_train, y_test = split_train_test(x, y, pred_horizon)\n",
    "        print(\"Shapes: xtr, xte, ytr, yte: \", x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "        preprocessingResult = [x_train, x_test, y_train, y_test, scalerX, scalerY]\n",
    "        with(open(preprocessingResultFile, \"wb\")) as pkl:\n",
    "            pickle.dump(preprocessingResult, pkl)\n",
    "    else:\n",
    "        with(open(preprocessingResultFile, \"rb\")) as pkl:\n",
    "            x_train, x_test, y_train, y_test, scalerX, scalerY = pickle.load(pkl)\n",
    "        numberOfFeatures = x_train.shape[2]\n",
    "            \n",
    "    paramDict[\"numberOfFeatures\"] = [numberOfFeatures];\n",
    "\n",
    "    model = KerasRegressor(build_fn = create_lstm)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n",
    "#     mc = ModelCheckpoint('grid_lstm_multistep_multivariate.h5', monitor='val_loss' , mode='min', verbose=1, save_best_only=True)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=paramDict, cv = 5, n_jobs=8)\n",
    "    grid_result = grid.fit(x_train, y_train, validation_split=0.1, epochs= 200, callbacks=[es], shuffle=False)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    fitTimes = grid_result.cv_results_['mean_fit_time']\n",
    "    for mean, stdev, param, fitTime in zip(means, stds, params, fitTimes):\n",
    "        print(\"%f (%f) with: %r, fitted in %f\" % (mean, stdev, param, fitTime))\n",
    "\n",
    "    print('''*** Model fitted ***''')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_data_preprocessing(df, n_history, pred_horizon, performPCA):\n",
    "    if not df.index.name == timestamp:\n",
    "        dataset = df.set_index(timestamp)\n",
    "    else:\n",
    "        dataset = df\n",
    "    \n",
    "    y = dataset[predictionColumn].copy()\n",
    "    x = dataset.drop(columns=predictionColumn)\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(x)\n",
    "    x = scalerX.transform(x)\n",
    "    scalerY = StandardScaler()\n",
    "   # .reshape(-1, 1) # needed for standardScaler\n",
    "    scalerY.fit(y.values.reshape(-1,1))\n",
    "    y = scalerY.transform(y.values.reshape(-1,1))\n",
    "    \n",
    "    if(performPCA):\n",
    "        pcaTransformer = PCA(0.95) # keep 95% variance\n",
    "        pcaTransformer.fit(x)\n",
    "        x = pcaTransformer.transform(x)\n",
    "        print(''' *** PCA Result***\\n Started with %d features, reduced to %d features''' \n",
    "              % (len(df.columns)-1, pcaTransformer.n_components_))\n",
    "        \n",
    "    transformed_df = pd.DataFrame().from_records(x)\n",
    "    transformed_df[predictionColumn] = y\n",
    "    \n",
    "    transformed_df.reset_index(inplace=True)\n",
    "    x, y = split_sequences(transformed_df.values, n_steps_in=n_history, n_steps_out=pred_horizon)\n",
    "    return x, y, scalerX, scalerY\n",
    "\n",
    "def gru_split_train_test(x, y, pred_horizon):\n",
    "    x_train = x[:-pred_horizon]\n",
    "    x_test = x[-pred_horizon:]\n",
    "    y_train = y[:-pred_horizon]\n",
    "    y_test = y[-pred_horizon:]\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def train_gru(df, n_history, pred_horizon, paramDict, performPCA=False):\n",
    "    preprocessingResultFile = \"grid_gru_preprocessingResult.pkl\"\n",
    "    if not os.path.isfile(preprocessingResultFile):\n",
    "        print(\"Preprocessing data...\")\n",
    "        # data split / preprocessing\n",
    "        x, y, scalerX, scalerY = gru_data_preprocessing(df, n_history=n_history, pred_horizon=pred_horizon, performPCA=performPCA)\n",
    "        print(\"SHAPES: \", x.shape, y.shape)\n",
    "        numberOfFeatures = x.shape[2]\n",
    "        x_train, x_test, y_train, y_test = split_train_test(x, y, pred_horizon)\n",
    "        print(\"Shapes: xtr, xte, ytr, yte: \", x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "        preprocessingResult = [x_train, x_test, y_train, y_test, scalerX, scalerY]\n",
    "        \n",
    "        with(open(preprocessingResultFile, \"wb\")) as pkl:\n",
    "            pickle.dump(preprocessingResult, pkl)\n",
    "    else:\n",
    "        with(open(preprocessingResultFile, \"rb\")) as pkl:\n",
    "            x_train, x_test, y_train, y_test, scalerX, scalerY = pickle.load(pkl)\n",
    "        numberOfFeatures = x_train.shape[2]\n",
    "   \n",
    "    paramDict[\"numberOfFeatures\"] = [numberOfFeatures];    \n",
    "        \n",
    "    model = KerasRegressor(build_fn = create_gru)\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "#     mc = ModelCheckpoint('grid_gru_multistep_multivariate.h5', monitor='val_loss' , mode='min', verbose=1, save_best_only=True)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=paramDict, cv = 5, n_jobs=8)\n",
    "    grid_result = grid.fit(x_train, y_train, validation_split=0.1, epochs= 100, callbacks=[es], shuffle=False)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    fitTimes = grid_result.cv_results_['mean_fit_time']\n",
    "    for mean, stdev, param, fitTime in zip(means, stds, params, fitTimes):\n",
    "        print(\"%f (%f) with: %r, fitted in %f\" % (mean, stdev, param, fitTime))\n",
    "\n",
    "    print('''*** Model fitted ***''')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_data_preprocessing(df, n_history, pred_horizon, performPCA):\n",
    "    if not df.index.name == timestamp:\n",
    "        dataset = df.set_index(timestamp)\n",
    "    else:\n",
    "        dataset = df\n",
    "         \n",
    "    y = dataset[predictionColumn].copy()\n",
    "    x = dataset.drop(columns=predictionColumn)\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(x)\n",
    "    x = scalerX.transform(x)\n",
    "    scalerY = StandardScaler()\n",
    "   # .reshape(-1, 1) # needed for standardScaler\n",
    "    scalerY.fit(y.values.reshape(-1,1))\n",
    "    y = scalerY.transform(y.values.reshape(-1,1))\n",
    "    \n",
    "    if(performPCA):\n",
    "        pcaTransformer = PCA(0.95) # keep 95% variance\n",
    "        pcaTransformer.fit(x)\n",
    "        x = pcaTransformer.transform(x)\n",
    "        print(''' *** PCA Result***\\n Started with %d features, reduced to %d features''' \n",
    "              % (len(df.columns)-1, pcaTransformer.n_components_))\n",
    "        \n",
    "    transformed_df = pd.DataFrame().from_records(x)\n",
    "    transformed_df[predictionColumn] = y\n",
    "    \n",
    "    transformed_df.reset_index(inplace=True)\n",
    "    x, y = split_sequences(transformed_df.values, n_steps_in=n_history, n_steps_out=pred_horizon)\n",
    "    return x, y, scalerX, scalerY\n",
    "\n",
    "\n",
    "def train_mlp(df, n_history, pred_horizon, paramDict, performPCA=True):\n",
    "    preprocessingResultFile = \"grid_mlp_preprocessingResult.pkl\"\n",
    "    if not os.path.isfile(preprocessingResultFile):\n",
    "        print(\"Preprocessing data...\")\n",
    "        x, y, scalerX, scalerY = mlp_data_preprocessing(df, n_history=n_history, pred_horizon=pred_horizon, performPCA=performPCA)\n",
    "        print(\"SHAPES: \", x.shape, y.shape)\n",
    "        x_train, x_test, y_train, y_test = split_train_test(x, y, pred_horizon)\n",
    "        print(\"Shapes: xtr, xte, ytr, yte: \", x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "        n_input = x_train.shape[1] * x_train.shape[2]\n",
    "        x_train = x_train.reshape((x_train.shape[0], n_input))\n",
    "        n_neurons = int((n_input + pred_horizon))\n",
    "        preprocessingResult = [n_input, x_train, x_test, y_train, y_test, scalerX, scalerY]\n",
    "        with(open(preprocessingResultFile, \"wb\")) as pkl:\n",
    "            pickle.dump(preprocessingResult, pkl)\n",
    "    else:\n",
    "        with(open(preprocessingResultFile, \"rb\")) as pkl:\n",
    "            n_input, x_train, x_test, y_train, y_test, scalerX, scalerY = pickle.load(pkl)\n",
    "        n_neurons = int((n_input + pred_horizon))\n",
    "        \n",
    "    model = KerasRegressor(build_fn = create_mlp)\n",
    "    \n",
    "    \n",
    "    paramDict[\"numberOfNeurons\"] = [n_neurons]\n",
    "    paramDict[\"n_input\"] = [n_input]\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "#     mc = ModelCheckpoint('grid_mlp_multistep_multivariate.h5', monitor='val_loss' , mode='min', verbose=1, save_best_only=True)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=paramDict, cv = 5, n_jobs=8)\n",
    "    grid_result = grid.fit(x_train, y_train, validation_split=0.1, epochs= 100, callbacks=[es], shuffle=False)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    fitTimes = grid_result.cv_results_['mean_fit_time']\n",
    "    for mean, stdev, param, fitTime in zip(means, stds, params, fitTimes):\n",
    "        print(\"%f (%f) with: %r, fitted in %f\" % (mean, stdev, param, fitTime))\n",
    "\n",
    "    print('''*** Model fitted ***''')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_preprocessing(df, n_history, pred_horizon, performPCA):\n",
    "    if not df.index.name == timestamp:\n",
    "        dataset = df.set_index(timestamp)\n",
    "    else:\n",
    "        dataset = df\n",
    "    \n",
    "    y = dataset[predictionColumn].copy()\n",
    "    x = dataset.drop(columns=predictionColumn)\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(x)\n",
    "    x = scalerX.transform(x)\n",
    "    scalerY = StandardScaler()\n",
    "   # .reshape(-1, 1) # needed for standardScaler\n",
    "    scalerY.fit(y.values.reshape(-1,1))\n",
    "    y = scalerY.transform(y.values.reshape(-1,1))\n",
    "    \n",
    "    if(performPCA):\n",
    "        pcaTransformer = PCA(0.95) # keep 95% variance\n",
    "        pcaTransformer.fit(x)\n",
    "        x = pcaTransformer.transform(x)\n",
    "        print(''' *** PCA Result***\\n Started with %d features, reduced to %d features''' \n",
    "              % (len(df.columns)-1, pcaTransformer.n_components_))\n",
    "        \n",
    "    transformed_df = pd.DataFrame().from_records(x)\n",
    "    transformed_df[predictionColumn] = y\n",
    "    \n",
    "    transformed_df.reset_index(inplace=True)\n",
    "    x, y = split_sequences(transformed_df.values, n_steps_in=n_history, n_steps_out=pred_horizon)\n",
    "    return x, y, scalerX, scalerY\n",
    "\n",
    "def train_cnn(df, pred_horizon, n_history,paramDict, performPCA=True):\n",
    "    preprocessingResultFile = \"grid_cnn_preprocessingResult.pkl\"\n",
    "    if not os.path.isfile(preprocessingResultFile):\n",
    "        print(\"Preprocessing data...\")\n",
    "        x, y, scalerX, scalerY = cnn_data_preprocessing(df, n_history=n_history, pred_horizon=pred_horizon, performPCA=performPCA)\n",
    "        print(\"SHAPES: \", x.shape, y.shape)\n",
    "        numberOfFeatures = x.shape[2]\n",
    "        x_train, x_test, y_train, y_test = split_train_test(x, y, pred_horizon)\n",
    "        print(\"Shapes: xtr, xte, ytr, yte: \", x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "        preprocessingResult = [x_train, x_test, y_train, y_test, scalerX, scalerY]\n",
    "        with(open(preprocessingResultFile, \"wb\")) as pkl:\n",
    "            pickle.dump(preprocessingResult, pkl)\n",
    "    else:\n",
    "        with(open(preprocessingResultFile, \"rb\")) as pkl:\n",
    "            x_train, x_test, y_train, y_test, scalerX, scalerY = pickle.load(pkl)\n",
    "        numberOfFeatures = x_train.shape[2]\n",
    "      \n",
    "    paramDict[\"numberOfFeatures\"] = [numberOfFeatures];\n",
    "    model = KerasRegressor(build_fn = create_cnn)\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "#     mc = ModelCheckpoint('grid_cnn_multistep_multivariate.h5', monitor='val_loss' , mode='min', verbose=1, save_best_only=True)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=paramDict, cv = 5, n_jobs=8)\n",
    "    grid_result = grid.fit(x_train, y_train, validation_split=0.1, epochs= 100, callbacks=[es], shuffle=False)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    print(\"grid.cv_results_\\n\\n\")\n",
    "    print(grid.cv_results_)\n",
    "    print(\"grid.best_estimator\\n\\n\")\n",
    "    print(grid.best_estimator_)\n",
    "    print(\"grid.best_params\\n\\n\")\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    fitTimes = grid_result.cv_results_['mean_fit_time']\n",
    "    for mean, stdev, param, fitTime in zip(means, stds, params, fitTimes):\n",
    "        print(\"%f (%f) with: %r, fitted in %f\" % (mean, stdev, param, fitTime))\n",
    "\n",
    "    print('''*** Model fitted ***''')   \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declaration\n",
    "skip_csv_rows = 0\n",
    "measureInterval = 15 #min\n",
    "daysToPredict = 5\n",
    "pred_horizon = (60//measureInterval) * 24 * daysToPredict #5 days (4*24*5), timestep = 15min\n",
    "hours_history = 8\n",
    "n_history = (60//measureInterval)*hours_history \n",
    "\n",
    "# Read data from pickle file\n",
    "with open(\"./4week_transformed_droppedErrors_filled.pkl\", \"rb\") as pickleFile:\n",
    "    df = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[predictionColumn] = pd.to_numeric(df[predictionColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramCellCount = [50, 75]\n",
    "paramHiddenLayers = [4,6,8]\n",
    "paramActivation = [\"tanh\"]\n",
    "paramHistory = [n_history]\n",
    "paramPredHorizon = [pred_horizon]\n",
    "\n",
    "paramDict = dict(numberOfHiddenLayers = paramHiddenLayers, numberOfCells = paramCellCount, activation = paramActivation, \n",
    "                 n_history = paramHistory, pred_horizon = paramPredHorizon)\n",
    "starttime = time.time()\n",
    "train_lstm(df.copy().reset_index(), n_history=n_history, pred_horizon=pred_horizon, paramDict = paramDict)\n",
    "print(\"training lstm took \", time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** PCA Result***\n",
      " Started with 182 features, reduced to 60 features\n",
      "SHAPES:  (2279, 32, 61) (2279, 480)\n",
      "Shapes: xtr, xte, ytr, yte:  (1799, 32, 61) (480, 32, 61) (1799, 480) (480, 480)\n",
      "Train on 1619 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      "1619/1619 [==============================] - 6s 4ms/step - loss: 1.3211 - mse: 1.3211 - val_loss: 0.8722 - val_mse: 0.8722\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87217, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 2/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3018 - mse: 1.3018 - val_loss: 0.8714 - val_mse: 0.8714\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.87217 to 0.87144, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 3/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3016 - mse: 1.3016 - val_loss: 0.8714 - val_mse: 0.8714\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.87144\n",
      "Epoch 4/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3014 - mse: 1.3014 - val_loss: 0.8759 - val_mse: 0.8759\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.87144\n",
      "Epoch 5/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3014 - mse: 1.3014 - val_loss: 0.8724 - val_mse: 0.8724\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.87144\n",
      "Epoch 6/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3012 - mse: 1.3012 - val_loss: 0.8762 - val_mse: 0.8762\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.87144\n",
      "Epoch 7/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3012 - mse: 1.3012 - val_loss: 0.8759 - val_mse: 0.8759\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.87144\n",
      "Epoch 8/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3010 - mse: 1.3010 - val_loss: 0.8750 - val_mse: 0.8750\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.87144\n",
      "Epoch 9/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3009 - mse: 1.3009 - val_loss: 0.8807 - val_mse: 0.8807\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.87144\n",
      "Epoch 10/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3008 - mse: 1.3008 - val_loss: 0.8860 - val_mse: 0.8860\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.87144\n",
      "Epoch 11/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3008 - mse: 1.3008 - val_loss: 0.8814 - val_mse: 0.8814\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.87144\n",
      "Epoch 12/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3006 - mse: 1.3006 - val_loss: 0.8803 - val_mse: 0.8803\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.87144\n",
      "Epoch 13/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3003 - mse: 1.3003 - val_loss: 0.8859 - val_mse: 0.8859\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.87144\n",
      "Epoch 14/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3002 - mse: 1.3002 - val_loss: 0.8834 - val_mse: 0.8834\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.87144\n",
      "Epoch 15/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.3002 - mse: 1.3002 - val_loss: 0.8926 - val_mse: 0.8926\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.87144\n",
      "Epoch 16/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.2998 - mse: 1.2998 - val_loss: 0.8942 - val_mse: 0.8942\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.87144\n",
      "Epoch 17/100\n",
      "1619/1619 [==============================] - 5s 3ms/step - loss: 1.2999 - mse: 1.2999 - val_loss: 0.8918 - val_mse: 0.8918\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.87144\n",
      "Epoch 00017: early stopping\n",
      "Best: -1.253749 using {'activation': 'relu', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 4, 'pred_horizon': 480}\n",
      "nan (nan) with: {'activation': 'tanh', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 4, 'pred_horizon': 480}, fitted in 412.789439\n",
      "-1.253749 (1.089363) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 4, 'pred_horizon': 480}, fitted in 518.959704\n",
      "*** Model fitted ***\n",
      "training GRU took  928.7071752548218\n"
     ]
    }
   ],
   "source": [
    "paramCellCount = [50]\n",
    "paramHiddenLayers = [4,5,7]\n",
    "paramActivation = [\"relu\"]\n",
    "paramHistory = [n_history]\n",
    "paramPredHorizon = [pred_horizon]\n",
    "\n",
    "paramDict = dict(numberOfHiddenLayers = paramHiddenLayers, numberOfCells = paramCellCount, activation = paramActivation, \n",
    "                 n_history = paramHistory, pred_horizon = paramPredHorizon)\n",
    "# Grid Search for activation function relu vs tanh. Best: relu\n",
    "starttime = time.time()\n",
    "train_gru(df.copy().reset_index(), pred_horizon=pred_horizon, n_history=n_history, paramDict = paramDict)\n",
    "print(\"training GRU took \", time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** PCA Result***\n",
      " Started with 182 features, reduced to 60 features\n",
      "SHAPES:  (2279, 32, 61) (2279, 480)\n",
      "Shapes: xtr, xte, ytr, yte:  (1799, 32, 61) (480, 32, 61) (1799, 480) (480, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning:\n",
      "\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1619 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.7027 - mse: 1.7027 - val_loss: 0.8741 - val_mse: 0.8741\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87413, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 2/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.3025 - mse: 1.3025 - val_loss: 0.8724 - val_mse: 0.8724\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.87413 to 0.87236, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 3/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.3018 - mse: 1.3018 - val_loss: 0.8715 - val_mse: 0.8715\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.87236 to 0.87146, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 4/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.3015 - mse: 1.3015 - val_loss: 0.8726 - val_mse: 0.8726\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.87146\n",
      "Epoch 5/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.3012 - mse: 1.3012 - val_loss: 0.8708 - val_mse: 0.8708\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.87146 to 0.87078, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 6/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.3003 - mse: 1.3003 - val_loss: 0.8688 - val_mse: 0.8688\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.87078 to 0.86879, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 7/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2977 - mse: 1.2977 - val_loss: 0.8672 - val_mse: 0.8672\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.86879 to 0.86720, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 8/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2911 - mse: 1.2911 - val_loss: 0.8559 - val_mse: 0.8559\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.86720 to 0.85594, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 9/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2815 - mse: 1.2815 - val_loss: 0.8256 - val_mse: 0.8256\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.85594 to 0.82564, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 10/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2626 - mse: 1.2626 - val_loss: 0.8113 - val_mse: 0.8113\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.82564 to 0.81135, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 11/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2587 - mse: 1.2587 - val_loss: 0.8297 - val_mse: 0.8297\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.81135\n",
      "Epoch 12/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2530 - mse: 1.2530 - val_loss: 0.8076 - val_mse: 0.8076\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.81135 to 0.80764, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 13/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2467 - mse: 1.2467 - val_loss: 0.7961 - val_mse: 0.7961\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.80764 to 0.79614, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 14/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2441 - mse: 1.2441 - val_loss: 0.7941 - val_mse: 0.7941\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.79614 to 0.79408, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 15/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2429 - mse: 1.2429 - val_loss: 0.7920 - val_mse: 0.7920\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.79408 to 0.79199, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 16/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2424 - mse: 1.2424 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.79199\n",
      "Epoch 17/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 0.7920 - val_mse: 0.7920\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.79199\n",
      "Epoch 18/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 0.7919 - val_mse: 0.7919\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.79199 to 0.79190, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 19/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.79190\n",
      "Epoch 20/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.79190\n",
      "Epoch 21/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 0.7920 - val_mse: 0.7920\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.79190\n",
      "Epoch 22/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2420 - mse: 1.2420 - val_loss: 0.7921 - val_mse: 0.7921\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.79190\n",
      "Epoch 23/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.79190\n",
      "Epoch 24/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.79190\n",
      "Epoch 25/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 0.7925 - val_mse: 0.7925\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.79190\n",
      "Epoch 26/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2420 - mse: 1.2420 - val_loss: 0.7925 - val_mse: 0.7925\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.79190\n",
      "Epoch 27/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2420 - mse: 1.2420 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.79190\n",
      "Epoch 28/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2420 - mse: 1.2420 - val_loss: 0.7931 - val_mse: 0.7931\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.79190\n",
      "Epoch 29/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 0.7946 - val_mse: 0.7946\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.79190\n",
      "Epoch 30/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2421 - mse: 1.2421 - val_loss: 0.7933 - val_mse: 0.7933\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.79190\n",
      "Epoch 31/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2422 - mse: 1.2422 - val_loss: 0.7918 - val_mse: 0.7918\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.79190 to 0.79180, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 32/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2418 - mse: 1.2418 - val_loss: 0.7928 - val_mse: 0.7928\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.79180\n",
      "Epoch 33/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2417 - mse: 1.2417 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.79180\n",
      "Epoch 34/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2416 - mse: 1.2416 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.79180\n",
      "Epoch 35/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2415 - mse: 1.2415 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.79180\n",
      "Epoch 36/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2486 - mse: 1.2486 - val_loss: 0.8099 - val_mse: 0.8099\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.79180\n",
      "Epoch 37/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2433 - mse: 1.2433 - val_loss: 0.7908 - val_mse: 0.7908\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.79180 to 0.79081, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 38/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2401 - mse: 1.2401 - val_loss: 0.7894 - val_mse: 0.7894\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.79081 to 0.78941, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 39/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2394 - mse: 1.2394 - val_loss: 0.7896 - val_mse: 0.7896\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.78941\n",
      "Epoch 40/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2389 - mse: 1.2389 - val_loss: 0.7901 - val_mse: 0.7901\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.78941\n",
      "Epoch 41/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2387 - mse: 1.2387 - val_loss: 0.7891 - val_mse: 0.7891\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.78941 to 0.78908, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 42/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2379 - mse: 1.2379 - val_loss: 0.7879 - val_mse: 0.7879\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.78908 to 0.78791, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 43/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2378 - mse: 1.2378 - val_loss: 0.7884 - val_mse: 0.7884\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.78791\n",
      "Epoch 44/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2377 - mse: 1.2377 - val_loss: 0.7888 - val_mse: 0.7888\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.78791\n",
      "Epoch 45/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2378 - mse: 1.2378 - val_loss: 0.7896 - val_mse: 0.7896\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.78791\n",
      "Epoch 46/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2378 - mse: 1.2378 - val_loss: 0.7887 - val_mse: 0.7887\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.78791\n",
      "Epoch 47/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2376 - mse: 1.2376 - val_loss: 0.7898 - val_mse: 0.7898\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.78791\n",
      "Epoch 48/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2376 - mse: 1.2376 - val_loss: 0.7894 - val_mse: 0.7894\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.78791\n",
      "Epoch 49/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2378 - mse: 1.2378 - val_loss: 0.8011 - val_mse: 0.8011\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.78791\n",
      "Epoch 50/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2388 - mse: 1.2388 - val_loss: 0.7909 - val_mse: 0.7909\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.78791\n",
      "Epoch 51/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2372 - mse: 1.2372 - val_loss: 0.7894 - val_mse: 0.7894\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.78791\n",
      "Epoch 52/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2363 - mse: 1.2363 - val_loss: 0.7860 - val_mse: 0.7860\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.78791 to 0.78602, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 53/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2359 - mse: 1.2359 - val_loss: 0.7884 - val_mse: 0.7884\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.78602\n",
      "Epoch 54/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2358 - mse: 1.2358 - val_loss: 0.7875 - val_mse: 0.7875\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.78602\n",
      "Epoch 55/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2357 - mse: 1.2357 - val_loss: 0.7867 - val_mse: 0.7867\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.78602\n",
      "Epoch 56/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2356 - mse: 1.2356 - val_loss: 0.7866 - val_mse: 0.7866\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.78602\n",
      "Epoch 57/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2355 - mse: 1.2355 - val_loss: 0.7886 - val_mse: 0.7886\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.78602\n",
      "Epoch 58/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2356 - mse: 1.2356 - val_loss: 0.7878 - val_mse: 0.7878\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.78602\n",
      "Epoch 59/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2354 - mse: 1.2354 - val_loss: 0.7876 - val_mse: 0.7876\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.78602\n",
      "Epoch 60/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2353 - mse: 1.2353 - val_loss: 0.7943 - val_mse: 0.7943\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.78602\n",
      "Epoch 61/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2353 - mse: 1.2353 - val_loss: 0.7888 - val_mse: 0.7888\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.78602\n",
      "Epoch 62/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2352 - mse: 1.2352 - val_loss: 0.7868 - val_mse: 0.7868\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.78602\n",
      "Epoch 63/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2348 - mse: 1.2348 - val_loss: 0.7900 - val_mse: 0.7900\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.78602\n",
      "Epoch 64/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2347 - mse: 1.2347 - val_loss: 0.7883 - val_mse: 0.7883\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.78602\n",
      "Epoch 65/100\n",
      "1619/1619 [==============================] - 3s 2ms/step - loss: 1.2345 - mse: 1.2345 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.78602\n",
      "Epoch 66/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2348 - mse: 1.2348 - val_loss: 0.7892 - val_mse: 0.7892\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.78602\n",
      "Epoch 67/100\n",
      "1619/1619 [==============================] - 4s 2ms/step - loss: 1.2345 - mse: 1.2345 - val_loss: 0.7888 - val_mse: 0.7888\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.78602\n",
      "Epoch 00067: early stopping\n",
      "Best: -1.192224 using {'activation': 'relu', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 2, 'pred_horizon': 480}\n",
      "nan (nan) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 1, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 2, 'pred_horizon': 480}, fitted in 95.151721\n",
      "nan (nan) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 10, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 2, 'pred_horizon': 480}, fitted in 252.770440\n",
      "nan (nan) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 30, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 2, 'pred_horizon': 480}, fitted in 389.258257\n",
      "-1.192224 (1.114220) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 2, 'pred_horizon': 480}, fitted in 889.242476\n",
      "-1.201217 (1.105482) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 80, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 2, 'pred_horizon': 480}, fitted in 889.128241\n",
      "*** Model fitted ***\n",
      "training GRU took  2291.817639350891\n"
     ]
    }
   ],
   "source": [
    "paramCellCount = [50]\n",
    "paramHiddenLayers = [4,5,7]\n",
    "paramActivation = [\"relu\"]\n",
    "paramHistory = [n_history]\n",
    "paramPredHorizon = [pred_horizon]\n",
    "\n",
    "paramDict = dict(numberOfHiddenLayers = paramHiddenLayers, numberOfCells = paramCellCount, activation = paramActivation, \n",
    "                 n_history = paramHistory, pred_horizon = paramPredHorizon)\n",
    "# Grid Search for cell count. Best: 50\n",
    "starttime = time.time()\n",
    "train_gru(df.copy().reset_index(), pred_horizon=pred_horizon, n_history=n_history, paramDict = paramDict)\n",
    "print(\"training GRU took \", time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** PCA Result***\n",
      " Started with 182 features, reduced to 60 features\n",
      "SHAPES:  (2279, 32, 61) (2279, 480)\n",
      "Shapes: xtr, xte, ytr, yte:  (1799, 32, 61) (480, 32, 61) (1799, 480) (480, 480)\n",
      "Train on 1619 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      "1619/1619 [==============================] - 8s 5ms/step - loss: 1.3026 - mse: 1.3026 - val_loss: 0.8710 - val_mse: 0.8710\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87099, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 2/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3019 - mse: 1.3019 - val_loss: 0.8715 - val_mse: 0.8715\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.87099\n",
      "Epoch 3/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3016 - mse: 1.3016 - val_loss: 0.8710 - val_mse: 0.8710\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.87099\n",
      "Epoch 4/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3015 - mse: 1.3015 - val_loss: 0.8713 - val_mse: 0.8713\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.87099\n",
      "Epoch 5/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3011 - mse: 1.3011 - val_loss: 0.8715 - val_mse: 0.8715\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.87099\n",
      "Epoch 6/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3010 - mse: 1.3010 - val_loss: 0.8701 - val_mse: 0.8701\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.87099 to 0.87012, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 7/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3011 - mse: 1.3011 - val_loss: 0.8750 - val_mse: 0.8750\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.87012\n",
      "Epoch 8/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2999 - mse: 1.2999 - val_loss: 0.8732 - val_mse: 0.8732\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.87012\n",
      "Epoch 9/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2993 - mse: 1.2993 - val_loss: 0.8740 - val_mse: 0.8740\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.87012\n",
      "Epoch 10/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3003 - mse: 1.3003 - val_loss: 0.8679 - val_mse: 0.8679\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.87012 to 0.86787, saving model to grid_gru_multistep_multivariate.h5\n",
      "Epoch 11/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3026 - mse: 1.3026 - val_loss: 0.8693 - val_mse: 0.8693\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.86787\n",
      "Epoch 12/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3026 - mse: 1.3026 - val_loss: 0.8718 - val_mse: 0.8718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.86787\n",
      "Epoch 13/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3021 - mse: 1.3021 - val_loss: 0.8719 - val_mse: 0.8719\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.86787\n",
      "Epoch 14/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3009 - mse: 1.3009 - val_loss: 0.8727 - val_mse: 0.8727\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.86787\n",
      "Epoch 15/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3001 - mse: 1.3001 - val_loss: 0.8767 - val_mse: 0.8767\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.86787\n",
      "Epoch 16/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2995 - mse: 1.2995 - val_loss: 0.8739 - val_mse: 0.8739\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.86787\n",
      "Epoch 17/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2980 - mse: 1.2980 - val_loss: 0.8762 - val_mse: 0.8762\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.86787\n",
      "Epoch 18/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2972 - mse: 1.2972 - val_loss: 0.8777 - val_mse: 0.8777\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.86787\n",
      "Epoch 19/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2968 - mse: 1.2968 - val_loss: 0.8824 - val_mse: 0.8824\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.86787\n",
      "Epoch 20/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2978 - mse: 1.2978 - val_loss: 0.8738 - val_mse: 0.8738\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.86787\n",
      "Epoch 21/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2969 - mse: 1.2969 - val_loss: 0.8911 - val_mse: 0.8911\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.86787\n",
      "Epoch 22/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2969 - mse: 1.2969 - val_loss: 0.8957 - val_mse: 0.8957\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.86787\n",
      "Epoch 23/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.2962 - mse: 1.2962 - val_loss: 0.8858 - val_mse: 0.8858\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.86787\n",
      "Epoch 24/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3033 - mse: 1.3033 - val_loss: 0.8709 - val_mse: 0.8709\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.86787\n",
      "Epoch 25/100\n",
      "1619/1619 [==============================] - 7s 4ms/step - loss: 1.3021 - mse: 1.3021 - val_loss: 0.8740 - val_mse: 0.8740\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.86787\n",
      "Epoch 00025: early stopping\n",
      "Best: -1.284117 using {'activation': 'relu', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 5, 'pred_horizon': 480}\n",
      "nan (nan) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 4, 'pred_horizon': 480}, fitted in 311.672380\n",
      "-1.284117 (1.100035) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 5, 'pred_horizon': 480}, fitted in 524.476942\n",
      "-1.302860 (1.167065) with: {'activation': 'relu', 'n_history': 32, 'numberOfCells': 50, 'numberOfFeatures': 61, 'numberOfHiddenLayers': 7, 'pred_horizon': 480}, fitted in 703.827212\n",
      "*** Model fitted ***\n",
      "training GRU took  1511.3409337997437\n"
     ]
    }
   ],
   "source": [
    "paramCellCount = [50]\n",
    "paramHiddenLayers = [4,5,7]\n",
    "paramActivation = [\"relu\"]\n",
    "paramHistory = [n_history]\n",
    "paramPredHorizon = [pred_horizon]\n",
    "\n",
    "paramDict = dict(numberOfHiddenLayers = paramHiddenLayers, numberOfCells = paramCellCount, activation = paramActivation, \n",
    "                 n_history = paramHistory, pred_horizon = paramPredHorizon)\n",
    "# Grid Search for hidden layer count. Best: 5\n",
    "starttime = time.time()\n",
    "train_gru(df.copy().reset_index(), pred_horizon=pred_horizon, n_history=n_history, paramDict = paramDict)\n",
    "print(\"training GRU took \", time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramHiddenLayers = [3]\n",
    "paramActivation = [\"sigmoid\", \"tanh\", \"relu\"]\n",
    "paramHistory = [n_history]\n",
    "paramPredHorizon = [pred_horizon]\n",
    "paramHistory = [n_history]\n",
    "paramPredHorizon = [pred_horizon]\n",
    "\n",
    "paramDict = dict(numberOfHiddenLayers = paramHiddenLayers, activation = paramActivation, \n",
    "                 n_history = paramHistory, pred_horizon = paramPredHorizon)\n",
    "# Grid Search for: activation func relu vs tanh vs sigmoid. Best: relu\n",
    "starttime = time.time()\n",
    "train_mlp(df.copy().reset_index(), pred_horizon=pred_horizon, n_history=n_history, paramDict = paramDict)\n",
    "print(\"training mlp took \", time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2050 samples, validate on 228 samples\n",
      "Epoch 1/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.2002 - mse: 1.2002 - val_loss: 0.4472 - val_mse: 0.4472\n",
      "Epoch 2/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.2000 - mse: 1.2000 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 3/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1994 - mse: 1.1994 - val_loss: 0.4443 - val_mse: 0.4443\n",
      "Epoch 4/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1993 - mse: 1.1993 - val_loss: 0.4444 - val_mse: 0.4444\n",
      "Epoch 5/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1994 - mse: 1.1994 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 6/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1993 - mse: 1.1993 - val_loss: 0.4445 - val_mse: 0.4445\n",
      "Epoch 7/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 0.4445 - val_mse: 0.4445\n",
      "Epoch 8/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 9/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4445 - val_mse: 0.4445\n",
      "Epoch 10/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1972 - mse: 1.1972 - val_loss: 0.4471 - val_mse: 0.4471\n",
      "Epoch 11/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1966 - mse: 1.1966 - val_loss: 0.4483 - val_mse: 0.4483\n",
      "Epoch 12/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1906 - mse: 1.1906 - val_loss: 0.4452 - val_mse: 0.4452\n",
      "Epoch 13/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1820 - mse: 1.1820 - val_loss: 0.4491 - val_mse: 0.4491\n",
      "Epoch 14/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1873 - mse: 1.1873 - val_loss: 0.4483 - val_mse: 0.4483\n",
      "Epoch 15/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1857 - mse: 1.1857 - val_loss: 0.4491 - val_mse: 0.4491\n",
      "Epoch 16/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1641 - mse: 1.1641 - val_loss: 0.4557 - val_mse: 0.4557\n",
      "Epoch 17/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1618 - mse: 1.1618 - val_loss: 0.3780 - val_mse: 0.3780\n",
      "Epoch 18/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1214 - mse: 1.1214 - val_loss: 0.3515 - val_mse: 0.3515\n",
      "Epoch 19/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0974 - mse: 1.0974 - val_loss: 0.3316 - val_mse: 0.3316\n",
      "Epoch 20/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0798 - mse: 1.0798 - val_loss: 0.3149 - val_mse: 0.3149\n",
      "Epoch 21/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0686 - mse: 1.0686 - val_loss: 0.3082 - val_mse: 0.3082\n",
      "Epoch 22/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0762 - mse: 1.0762 - val_loss: 0.3053 - val_mse: 0.3053\n",
      "Epoch 23/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0655 - mse: 1.0655 - val_loss: 0.3001 - val_mse: 0.3001\n",
      "Epoch 24/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0525 - mse: 1.0525 - val_loss: 0.2887 - val_mse: 0.2887\n",
      "Epoch 25/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0500 - mse: 1.0500 - val_loss: 0.2976 - val_mse: 0.2976\n",
      "Epoch 26/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0448 - mse: 1.0448 - val_loss: 0.2843 - val_mse: 0.2843\n",
      "Epoch 27/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0412 - mse: 1.0412 - val_loss: 0.2837 - val_mse: 0.2837\n",
      "Epoch 28/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0335 - mse: 1.0335 - val_loss: 0.2701 - val_mse: 0.2701\n",
      "Epoch 29/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0281 - mse: 1.0281 - val_loss: 0.2702 - val_mse: 0.2702\n",
      "Epoch 30/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0229 - mse: 1.0229 - val_loss: 0.2568 - val_mse: 0.2568\n",
      "Epoch 31/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0154 - mse: 1.0154 - val_loss: 0.2508 - val_mse: 0.2508\n",
      "Epoch 32/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0088 - mse: 1.0088 - val_loss: 0.2459 - val_mse: 0.2459\n",
      "Epoch 33/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.0034 - mse: 1.0034 - val_loss: 0.2413 - val_mse: 0.2413\n",
      "Epoch 34/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9969 - mse: 0.9969 - val_loss: 0.2330 - val_mse: 0.2330\n",
      "Epoch 35/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9918 - mse: 0.9918 - val_loss: 0.2286 - val_mse: 0.2286\n",
      "Epoch 36/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9875 - mse: 0.9875 - val_loss: 0.2229 - val_mse: 0.2229\n",
      "Epoch 37/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9784 - mse: 0.9784 - val_loss: 0.2127 - val_mse: 0.2127\n",
      "Epoch 38/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9706 - mse: 0.9706 - val_loss: 0.2043 - val_mse: 0.2043\n",
      "Epoch 39/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9630 - mse: 0.9630 - val_loss: 0.2021 - val_mse: 0.2021\n",
      "Epoch 40/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9584 - mse: 0.9584 - val_loss: 0.1992 - val_mse: 0.1992\n",
      "Epoch 41/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9555 - mse: 0.9555 - val_loss: 0.1912 - val_mse: 0.1912\n",
      "Epoch 42/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9467 - mse: 0.9467 - val_loss: 0.1840 - val_mse: 0.1840\n",
      "Epoch 43/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9398 - mse: 0.9398 - val_loss: 0.1790 - val_mse: 0.1790\n",
      "Epoch 44/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9330 - mse: 0.9330 - val_loss: 0.1732 - val_mse: 0.1732\n",
      "Epoch 45/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9276 - mse: 0.9276 - val_loss: 0.1689 - val_mse: 0.1689\n",
      "Epoch 46/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9222 - mse: 0.9222 - val_loss: 0.1643 - val_mse: 0.1643\n",
      "Epoch 47/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9174 - mse: 0.9174 - val_loss: 0.1582 - val_mse: 0.1582\n",
      "Epoch 48/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9128 - mse: 0.9128 - val_loss: 0.1537 - val_mse: 0.1537\n",
      "Epoch 49/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9083 - mse: 0.9083 - val_loss: 0.1540 - val_mse: 0.1540\n",
      "Epoch 50/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9030 - mse: 0.9030 - val_loss: 0.1465 - val_mse: 0.1465\n",
      "Epoch 51/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8976 - mse: 0.8976 - val_loss: 0.1430 - val_mse: 0.1430\n",
      "Epoch 52/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8925 - mse: 0.8925 - val_loss: 0.1378 - val_mse: 0.1378\n",
      "Epoch 53/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8867 - mse: 0.8867 - val_loss: 0.1335 - val_mse: 0.1335\n",
      "Epoch 54/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8813 - mse: 0.8813 - val_loss: 0.1276 - val_mse: 0.1276\n",
      "Epoch 55/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8758 - mse: 0.8758 - val_loss: 0.1213 - val_mse: 0.1213\n",
      "Epoch 56/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8700 - mse: 0.8700 - val_loss: 0.1168 - val_mse: 0.1168\n",
      "Epoch 57/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8637 - mse: 0.8637 - val_loss: 0.1089 - val_mse: 0.1089\n",
      "Epoch 58/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8556 - mse: 0.8556 - val_loss: 0.1024 - val_mse: 0.1024\n",
      "Epoch 59/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8486 - mse: 0.8486 - val_loss: 0.0968 - val_mse: 0.0968\n",
      "Epoch 60/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.8428 - mse: 0.8428 - val_loss: 0.0987 - val_mse: 0.0987\n",
      "Epoch 61/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 0.9161 - mse: 0.9161 - val_loss: 0.5164 - val_mse: 0.5164\n",
      "Epoch 62/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1816 - mse: 1.1816 - val_loss: 0.4574 - val_mse: 0.4574\n",
      "Epoch 63/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.2047 - mse: 1.2047 - val_loss: 0.4487 - val_mse: 0.4487\n",
      "Epoch 64/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.2020 - mse: 1.2020 - val_loss: 0.4459 - val_mse: 0.4459\n",
      "Epoch 65/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.2001 - mse: 1.2001 - val_loss: 0.4457 - val_mse: 0.4457\n",
      "Epoch 66/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1830 - mse: 1.1830 - val_loss: 0.4539 - val_mse: 0.4539\n",
      "Epoch 67/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.2059 - mse: 1.2059 - val_loss: 0.4577 - val_mse: 0.4577\n",
      "Epoch 68/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.2061 - mse: 1.2061 - val_loss: 0.4584 - val_mse: 0.4584\n",
      "Epoch 69/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.2027 - mse: 1.2027 - val_loss: 0.4568 - val_mse: 0.4568\n",
      "Epoch 70/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1949 - mse: 1.1949 - val_loss: 0.4503 - val_mse: 0.4503\n",
      "Epoch 71/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1779 - mse: 1.1779 - val_loss: 0.4219 - val_mse: 0.4219\n",
      "Epoch 72/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1632 - mse: 1.1632 - val_loss: 0.4039 - val_mse: 0.4039\n",
      "Epoch 73/100\n",
      "2050/2050 [==============================] - 3s 1ms/step - loss: 1.1620 - mse: 1.1620 - val_loss: 0.4017 - val_mse: 0.4017\n",
      "Epoch 74/100\n",
      "2050/2050 [==============================] - 2s 1ms/step - loss: 1.1408 - mse: 1.1408 - val_loss: 0.4431 - val_mse: 0.4431\n",
      "Epoch 00074: early stopping\n",
      "Best: -1.127347 using {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}\n",
      "grid.cv_results_\n",
      "\n",
      "\n",
      "{'mean_fit_time': array([136.95713086]), 'std_fit_time': array([58.96245217]), 'mean_score_time': array([0.259904]), 'std_score_time': array([0.07594142]), 'param_activation': masked_array(data=['tanh'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_filters': masked_array(data=[(150, 40)],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernelSize': masked_array(data=[2],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_lstmActivation': masked_array(data=['tanh'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_history': masked_array(data=[32],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_numberOfFeatures': masked_array(data=[61],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_numberOfHiddenConvPoolLayers': masked_array(data=[2],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_numberOfLstmCells': masked_array(data=[75],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_numberOfLstmLayers': masked_array(data=[1],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_poolSize': masked_array(data=[2],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_pred_horizon': masked_array(data=[480],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}], 'split0_test_score': array([-0.41241373]), 'split1_test_score': array([-0.40954807]), 'split2_test_score': array([-2.53288814]), 'split3_test_score': array([-1.68294902]), 'split4_test_score': array([-0.59893827]), 'mean_test_score': array([-1.12734745]), 'std_test_score': array([0.84732759]), 'rank_test_score': array([1])}\n",
      "grid.best_estimator\n",
      "\n",
      "\n",
      "<keras.wrappers.scikit_learn.KerasRegressor object at 0x000002258F64BA88>\n",
      "grid.best_params\n",
      "\n",
      "\n",
      "{'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}\n",
      "-1.127347 (0.847328) with: {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}, fitted in 136.957131\n",
      "*** Model fitted ***\n",
      "training cnn took  400.006139755249\n"
     ]
    }
   ],
   "source": [
    "paramNumberOfHiddenConvPoolLayers = [2]\n",
    "paramFilters = [(150,40)]# tuples instead of 2d array [[],[]] because of a bug: https://github.com/keras-team/keras/issues/13586\n",
    "paramKernelSize = [2]\n",
    "paramPoolSize = [2]\n",
    "paramActivation = [\"tanh\"]\n",
    "paramNumberOfLstmLayers = [1]\n",
    "paramNumberOfLstmCells = [75]\n",
    "paramLstmActivation = [\"tanh\"]\n",
    "paramHistory = [n_history]\n",
    "paramPredHorizon = [pred_horizon]\n",
    "\n",
    "paramDict = dict(numberOfHiddenConvPoolLayers = paramNumberOfHiddenConvPoolLayers, filters=paramFilters, kernelSize = paramKernelSize,\n",
    "                poolSize = paramPoolSize, activation=paramActivation,\n",
    "                numberOfLstmLayers = paramNumberOfLstmLayers, numberOfLstmCells=paramNumberOfLstmCells, lstmActivation=paramLstmActivation,\n",
    "                n_history = paramHistory, pred_horizon = paramPredHorizon)\n",
    "# Grid Search for: KernelSize, PoolSize Best: kernelSize=2, poolSize=2\n",
    "starttime = time.time()\n",
    "train_cnn(df.copy().reset_index(), pred_horizon=pred_horizon, n_history=n_history, paramDict = paramDict)\n",
    "print(\"training cnn took \", time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2050 samples, validate on 228 samples\n",
      "Epoch 1/100\n",
      "2050/2050 [==============================] - 1s 547us/step - loss: 1.2001 - mse: 1.2001 - val_loss: 0.4493 - val_mse: 0.4493\n",
      "Epoch 2/100\n",
      "2050/2050 [==============================] - 1s 338us/step - loss: 1.2011 - mse: 1.2011 - val_loss: 0.4466 - val_mse: 0.4466\n",
      "Epoch 3/100\n",
      "2050/2050 [==============================] - 1s 357us/step - loss: 1.2002 - mse: 1.2002 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 4/100\n",
      "2050/2050 [==============================] - 1s 348us/step - loss: 1.1995 - mse: 1.1995 - val_loss: 0.4445 - val_mse: 0.4445\n",
      "Epoch 5/100\n",
      "2050/2050 [==============================] - 1s 352us/step - loss: 1.1994 - mse: 1.1994 - val_loss: 0.4445 - val_mse: 0.4445\n",
      "Epoch 6/100\n",
      "2050/2050 [==============================] - 1s 351us/step - loss: 1.1993 - mse: 1.1993 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 7/100\n",
      "2050/2050 [==============================] - 1s 350us/step - loss: 1.1993 - mse: 1.1993 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 8/100\n",
      "2050/2050 [==============================] - 1s 355us/step - loss: 1.1993 - mse: 1.1993 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 9/100\n",
      "2050/2050 [==============================] - 1s 353us/step - loss: 1.1992 - mse: 1.1992 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 10/100\n",
      "2050/2050 [==============================] - 1s 352us/step - loss: 1.1992 - mse: 1.1992 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 11/100\n",
      "2050/2050 [==============================] - 1s 358us/step - loss: 1.1992 - mse: 1.1992 - val_loss: 0.4447 - val_mse: 0.4447\n",
      "Epoch 12/100\n",
      "2050/2050 [==============================] - 1s 364us/step - loss: 1.1991 - mse: 1.1992 - val_loss: 0.4447 - val_mse: 0.4447\n",
      "Epoch 13/100\n",
      "2050/2050 [==============================] - 1s 350us/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4447 - val_mse: 0.4447\n",
      "Epoch 14/100\n",
      "2050/2050 [==============================] - 1s 354us/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4447 - val_mse: 0.4447\n",
      "Epoch 15/100\n",
      "2050/2050 [==============================] - 1s 354us/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4447 - val_mse: 0.4447\n",
      "Epoch 16/100\n",
      "2050/2050 [==============================] - 1s 353us/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4448 - val_mse: 0.4448\n",
      "Epoch 17/100\n",
      "2050/2050 [==============================] - 1s 363us/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4448 - val_mse: 0.4448\n",
      "Epoch 18/100\n",
      "2050/2050 [==============================] - 1s 360us/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4448 - val_mse: 0.4448\n",
      "Epoch 19/100\n",
      "2050/2050 [==============================] - 1s 357us/step - loss: 1.1991 - mse: 1.1991 - val_loss: 0.4448 - val_mse: 0.4448\n",
      "Epoch 00019: early stopping\n",
      "Best: -1.126539 using {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 3, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}\n",
      "grid.cv_results_\n",
      "\n",
      "\n",
      "{'mean_fit_time': array([  0.52848301,  72.21127391, 115.60849996,   2.55459943,\n",
      "        76.89535131,  72.1524456 ]), 'std_fit_time': array([1.22646488e-02, 3.48962682e+01, 1.33110249e+01, 1.25500622e+00,\n",
      "       1.96818277e+00, 1.05034176e+01]), 'mean_score_time': array([0.        , 0.24240174, 0.20100451, 0.        , 0.15679832,\n",
      "       0.10539932]), 'std_score_time': array([0.        , 0.05112903, 0.05404377, 0.        , 0.05199101,\n",
      "       0.02943271]), 'param_activation': masked_array(data=['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh'],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_filters': masked_array(data=[(150, 40), (150, 40), (150, 40), (150, 40), (150, 40),\n",
      "                   (150, 40)],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernelSize': masked_array(data=[2, 2, 2, 2, 2, 2],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_lstmActivation': masked_array(data=['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh'],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_history': masked_array(data=[32, 32, 32, 32, 32, 32],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_numberOfFeatures': masked_array(data=[61, 61, 61, 61, 61, 61],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_numberOfHiddenConvPoolLayers': masked_array(data=[2, 2, 2, 3, 3, 3],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_numberOfLstmCells': masked_array(data=[75, 75, 75, 75, 75, 75],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_numberOfLstmLayers': masked_array(data=[0, 1, 2, 0, 1, 2],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_poolSize': masked_array(data=[2, 2, 2, 2, 2, 2],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_pred_horizon': masked_array(data=[480, 480, 480, 480, 480, 480],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 0, 'poolSize': 2, 'pred_horizon': 480}, {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}, {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 2, 'poolSize': 2, 'pred_horizon': 480}, {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 3, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 0, 'poolSize': 2, 'pred_horizon': 480}, {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 3, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}, {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 3, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 2, 'poolSize': 2, 'pred_horizon': 480}], 'split0_test_score': array([        nan, -0.41240758, -0.41258539,         nan, -0.41230734,\n",
      "       -0.41247445]), 'split1_test_score': array([        nan, -0.4099219 , -0.41009258,         nan, -0.40986739,\n",
      "       -0.40995829]), 'split2_test_score': array([        nan, -2.55233987, -2.5236812 ,         nan, -2.52363393,\n",
      "       -2.52366064]), 'split3_test_score': array([        nan, -1.68799916, -1.68789299,         nan, -1.68792314,\n",
      "       -1.68790859]), 'split4_test_score': array([        nan, -0.598937  , -0.59897066,         nan, -0.59896319,\n",
      "       -0.59897217]), 'mean_test_score': array([        nan, -1.1323211 , -1.12664457,         nan, -1.126539  ,\n",
      "       -1.12659483]), 'std_test_score': array([       nan, 0.8543847 , 0.84480508,        nan, 0.8448796 ,\n",
      "       0.84484171]), 'rank_test_score': array([5, 4, 3, 6, 1, 2])}\n",
      "grid.best_estimator\n",
      "\n",
      "\n",
      "<keras.wrappers.scikit_learn.KerasRegressor object at 0x000002258B4D1148>\n",
      "grid.best_params\n",
      "\n",
      "\n",
      "{'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 3, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}\n",
      "nan (nan) with: {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 0, 'poolSize': 2, 'pred_horizon': 480}, fitted in 0.528483\n",
      "-1.132321 (0.854385) with: {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}, fitted in 72.211274\n",
      "-1.126645 (0.844805) with: {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 2, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 2, 'poolSize': 2, 'pred_horizon': 480}, fitted in 115.608500\n",
      "nan (nan) with: {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 3, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 0, 'poolSize': 2, 'pred_horizon': 480}, fitted in 2.554599\n",
      "-1.126539 (0.844880) with: {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 3, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 1, 'poolSize': 2, 'pred_horizon': 480}, fitted in 76.895351\n",
      "-1.126595 (0.844842) with: {'activation': 'tanh', 'filters': (150, 40), 'kernelSize': 2, 'lstmActivation': 'tanh', 'n_history': 32, 'numberOfFeatures': 61, 'numberOfHiddenConvPoolLayers': 3, 'numberOfLstmCells': 75, 'numberOfLstmLayers': 2, 'poolSize': 2, 'pred_horizon': 480}, fitted in 72.152446\n",
      "*** Model fitted ***\n",
      "training cnn took  271.34802889823914\n"
     ]
    }
   ],
   "source": [
    "paramNumberOfHiddenConvPoolLayers = [2, 3]\n",
    "paramFilters = [(150,40)]# tuples instead of 2d array [[],[]] because of a bug: https://github.com/keras-team/keras/issues/13586\n",
    "paramKernelSize = [2]\n",
    "paramPoolSize = [2]\n",
    "paramActivation = [\"tanh\"]\n",
    "paramNumberOfLstmLayers = [0,1,2]\n",
    "paramNumberOfLstmCells = [75]\n",
    "paramLstmActivation = [\"tanh\"]\n",
    "paramHistory = [n_history]\n",
    "paramPredHorizon = [pred_horizon]\n",
    "\n",
    "paramDict = dict(numberOfHiddenConvPoolLayers = paramNumberOfHiddenConvPoolLayers, filters=paramFilters, kernelSize = paramKernelSize,\n",
    "                poolSize = paramPoolSize, activation=paramActivation,\n",
    "                numberOfLstmLayers = paramNumberOfLstmLayers, numberOfLstmCells=paramNumberOfLstmCells, lstmActivation=paramLstmActivation,\n",
    "                n_history = paramHistory, pred_horizon = paramPredHorizon)\n",
    "# Grid Search for: KernelSize, PoolSize Best: kernelSize=2, poolSize=2\n",
    "starttime = time.time()\n",
    "train_cnn(df.copy().reset_index(), pred_horizon=pred_horizon, n_history=n_history, paramDict = paramDict)\n",
    "print(\"training cnn took \", time.time() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
