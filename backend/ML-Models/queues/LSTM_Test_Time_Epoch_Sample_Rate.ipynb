{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for time length of epochs & sample rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from training_functions import *\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch Length Tests with a sample rate of 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data from ES and create a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take datapoints in the range of the 9th and 13th of June with a sample rate of 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.391s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.344s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.451s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.166s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.387s]\n",
      "INFO:root:ES to Df: 2.11 time elapsed\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.115s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.749s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.732s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.141s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.189s]\n",
      "INFO:root:ES to Df: 2.56 time elapsed\n"
     ]
    }
   ],
   "source": [
    "q_one = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=20, tier=\"censhare\", host='localhost', port=9200)\n",
    "q_two = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=20, tier=\"pic\", host='localhost', port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the feature enriched dataset for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:3215 items in the first and second queue\n",
      "INFO:root:11177 items in the second queue only\n",
      "INFO:root:14392 items in the whole dataset\n",
      "INFO:root:Create dataset: 19.75 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X, y, maxlen = create_dataset_train(q_one, q_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling contains dummy inputs, which are only used for naming purposes of the scaler, which isnÂ´t used here for testing the MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scale: 68.85 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_scaled, y_scaled = scale(X, y, start_date=\"2020-11-03\",end_date=\"2020-11-07\", \n",
    "                           epochs='50', steps='720', s_rate='20', model_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad Split: 0.26 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pad_split(X_scaled, y_scaled, maxlen, test_size=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downsample: 0.06 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = downsample(X_train, X_test, y_train, y_test, rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the trainings samples (number of samples, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14104, 720, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Length Test with approx. 14000 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "353/353 [==============================] - 203s 576ms/step - loss: 0.0594 - mae: 0.5596 - val_loss: 0.0515 - val_mae: 0.4604\n",
      "Epoch 2/2\n",
      "353/353 [==============================] - 200s 568ms/step - loss: 0.0497 - mae: 0.4681 - val_loss: 0.0477 - val_mae: 0.4266\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n_steps = X_train_sampled.shape[1]  # number of steps\n",
    "n_features = X_train_sampled.shape[2]  # number of features\n",
    "\n",
    "# Create model layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(n_steps, n_features)))  # Masking Layer for padding\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(1, input_dim=20))  # Dense Layer to generate 1Dimensional Outputs\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define CallBacks\n",
    "early_stop = EarlyStopping(monitor='mae', mode='min', patience=10)\n",
    "mcp_save = ModelCheckpoint(f'models/mcp_lstm_2epochs_20neurons_14000items.h5', \n",
    "                            save_best_only=True, monitor='mae', mode='min')\n",
    "\n",
    "# Start training\n",
    "model.fit(X_train_sampled, y_train_sampled, epochs=2, validation_split=0.2,\n",
    "          callbacks=[early_stop, mcp_save])\n",
    "\n",
    "# Save the model in models directory\n",
    "model.save(f'models/lstm_lstm_2epochs_20neurons_14000items.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Length Test with approx. 11000 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad Split: 0.23 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pad_split(X_scaled, y_scaled, maxlen, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downsample: 0.06 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = downsample(X_train, X_test, y_train, y_test, rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10794, 720, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "270/270 [==============================] - 154s 570ms/step - loss: 0.0602 - mae: 0.5631 - val_loss: 0.0522 - val_mae: 0.4900\n",
      "Epoch 2/2\n",
      "270/270 [==============================] - 152s 564ms/step - loss: 0.0502 - mae: 0.4695 - val_loss: 0.0458 - val_mae: 0.4302\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n_steps = X_train_sampled.shape[1]  # number of steps\n",
    "n_features = X_train_sampled.shape[2]  # number of features\n",
    "\n",
    "# Create model layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(n_steps, n_features)))  # Masking Layer for padding\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(1, input_dim=20))  # Dense Layer to generate 1Dimensional Outputs\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define CallBacks\n",
    "early_stop = EarlyStopping(monitor='mae', mode='min', patience=10)\n",
    "mcp_save = ModelCheckpoint(f'models/mcp_lstm_2epochs_20neurons_11000items.h5', \n",
    "                            save_best_only=True, monitor='mae', mode='min')\n",
    "\n",
    "# Start training\n",
    "model.fit(X_train_sampled, y_train_sampled, epochs=2, validation_split=0.2,\n",
    "          callbacks=[early_stop, mcp_save])\n",
    "\n",
    "# Save the model in models directory\n",
    "model.save(f'models/lstm_lstm_2epochs_20neurons_11000items.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Length Test with approx. 7000 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad Split: 0.26 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pad_split(X_scaled, y_scaled, maxlen, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downsample: 0.06 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = downsample(X_train, X_test, y_train, y_test, rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7196, 720, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "180/180 [==============================] - 104s 580ms/step - loss: 0.0611 - mae: 0.5637 - val_loss: 0.0588 - val_mae: 0.5369\n",
      "Epoch 2/2\n",
      "180/180 [==============================] - 103s 570ms/step - loss: 0.0533 - mae: 0.4919 - val_loss: 0.0543 - val_mae: 0.4957\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n_steps = X_train_sampled.shape[1]  # number of steps\n",
    "n_features = X_train_sampled.shape[2]  # number of features\n",
    "\n",
    "# Create model layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(n_steps, n_features)))  # Masking Layer for padding\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(1, input_dim=20))  # Dense Layer to generate 1Dimensional Outputs\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define CallBacks\n",
    "early_stop = EarlyStopping(monitor='mae', mode='min', patience=10)\n",
    "mcp_save = ModelCheckpoint(f'models/mcp_lstm_2epochs_20neurons_7000items.h5', \n",
    "                            save_best_only=True, monitor='mae', mode='min')\n",
    "\n",
    "# Start training\n",
    "model.fit(X_train_sampled, y_train_sampled, epochs=2, validation_split=0.2,\n",
    "          callbacks=[early_stop, mcp_save])\n",
    "\n",
    "# Save the model in models directory\n",
    "model.save(f'models/lstm_lstm_2epochs_20neurons_7000items.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Length Test with approx. 3500 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad Split: 0.25 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pad_split(X_scaled, y_scaled, maxlen, test_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downsample: 0.06 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = downsample(X_train, X_test, y_train, y_test, rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3598, 720, 6)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "90/90 [==============================] - 55s 608ms/step - loss: 0.0645 - mae: 0.6161 - val_loss: 0.0637 - val_mae: 0.5981\n",
      "Epoch 2/2\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 0.0561 - mae: 0.5364 - val_loss: 0.0604 - val_mae: 0.5667\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n_steps = X_train_sampled.shape[1]  # number of steps\n",
    "n_features = X_train_sampled.shape[2]  # number of features\n",
    "\n",
    "# Create model layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(n_steps, n_features)))  # Masking Layer for padding\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(1, input_dim=20))  # Dense Layer to generate 1Dimensional Outputs\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define CallBacks\n",
    "early_stop = EarlyStopping(monitor='mae', mode='min', patience=10)\n",
    "mcp_save = ModelCheckpoint(f'models/mcp_lstm_2epochs_20neurons_3500items.h5', \n",
    "                            save_best_only=True, monitor='mae', mode='min')\n",
    "\n",
    "# Start training\n",
    "model.fit(X_train_sampled, y_train_sampled, epochs=2, validation_split=0.2,\n",
    "          callbacks=[early_stop, mcp_save])\n",
    "\n",
    "# Save the model in models directory\n",
    "model.save(f'models/lstm_lstm_2epochs_20neurons_3500items.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Rate Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Length Test Sample Rate 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.092s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.080s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.091s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.128s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.210s]\n",
      "INFO:root:ES to Df: 1.12 time elapsed\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.116s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.785s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.658s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.166s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.196s]\n",
      "INFO:root:ES to Df: 3.75 time elapsed\n"
     ]
    }
   ],
   "source": [
    "q_one = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=1, tier=\"censhare\", host='localhost', port=9200)\n",
    "q_two = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=1, tier=\"pic\", host='localhost', port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the feature enriched dataset for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:5598 items in the first and second queue\n",
      "INFO:root:11644 items in the second queue only\n",
      "INFO:root:17242 items in the whole dataset\n",
      "INFO:root:Create dataset: 56.96 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X, y, maxlen = create_dataset_train(q_one, q_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scale: 81.99 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_scaled, y_scaled = scale(X, y, start_date=\"2020-11-03\",end_date=\"2020-11-07\", \n",
    "                           epochs='50', steps='720', s_rate='20', model_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad Split: 19.08 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pad_split(X_scaled, y_scaled, maxlen, test_size=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downsample: 8.24 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = downsample(X_train, X_test, y_train, y_test, rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17240, 14400, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431/431 [==============================] - 11435s 27s/step - loss: 0.0668 - mae: 0.5841 - val_loss: 0.0690 - val_mae: 0.6041\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n_steps = X_train_sampled.shape[1]  # number of steps\n",
    "n_features = X_train_sampled.shape[2]  # number of features\n",
    "\n",
    "# Create model layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(n_steps, n_features)))  # Masking Layer for padding\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(1, input_dim=20))  # Dense Layer to generate 1Dimensional Outputs\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define CallBacks\n",
    "early_stop = EarlyStopping(monitor='mae', mode='min', patience=10)\n",
    "mcp_save = ModelCheckpoint(f'models/mcp_lstm_2epochs_20neurons_10000items_1srate.h5', \n",
    "                            save_best_only=True, monitor='mae', mode='min')\n",
    "\n",
    "# Start training\n",
    "model.fit(X_train_sampled, y_train_sampled, epochs=1, validation_split=0.2,\n",
    "          callbacks=[early_stop, mcp_save])\n",
    "\n",
    "# Save the model in models directory\n",
    "model.save(f'models/lstm_lstm_2epochs_20neurons_10000items_1srate.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Length Test Sample Rate 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.135s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.363s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.387s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.229s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.306s]\n",
      "INFO:root:ES to Df: 1.93 time elapsed\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.115s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.853s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.713s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.153s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.195s]\n",
      "INFO:root:ES to Df: 2.82 time elapsed\n"
     ]
    }
   ],
   "source": [
    "q_one = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=5, tier=\"censhare\", host='localhost', port=9200)\n",
    "q_two = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=5, tier=\"pic\", host='localhost', port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the feature enriched dataset for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:4343 items in the first and second queue\n",
      "INFO:root:10937 items in the second queue only\n",
      "INFO:root:15280 items in the whole dataset\n",
      "INFO:root:Create dataset: 26.72 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X, y, maxlen = create_dataset_train(q_one, q_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scale: 70.59 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_scaled, y_scaled = scale(X, y, start_date=\"2020-11-03\",end_date=\"2020-11-07\", \n",
    "                           epochs='50', steps='720', s_rate='20', model_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad Split: 0.73 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pad_split(X_scaled, y_scaled, maxlen, test_size=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downsample: 0.28 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = downsample(X_train, X_test, y_train, y_test, rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15278, 2880, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 1019s 3s/step - loss: 0.0580 - mae: 0.5433 - val_loss: 0.0548 - val_mae: 0.5033\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n_steps = X_train_sampled.shape[1]  # number of steps\n",
    "n_features = X_train_sampled.shape[2]  # number of features\n",
    "\n",
    "# Create model layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(n_steps, n_features)))  # Masking Layer for padding\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(1, input_dim=20))  # Dense Layer to generate 1Dimensional Outputs\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define CallBacks\n",
    "early_stop = EarlyStopping(monitor='mae', mode='min', patience=10)\n",
    "mcp_save = ModelCheckpoint(f'models/mcp_lstm_2epochs_20neurons_10000items_5srate.h5', \n",
    "                            save_best_only=True, monitor='mae', mode='min')\n",
    "\n",
    "# Start training\n",
    "model.fit(X_train_sampled, y_train_sampled, epochs=1, validation_split=0.2,\n",
    "          callbacks=[early_stop, mcp_save])\n",
    "\n",
    "# Save the model in models directory\n",
    "model.save(f'models/lstm_lstm_2epochs_20neurons_10000items_5srate.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Length Test Sample Rate 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.094s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.094s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.089s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.121s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.208s]\n",
      "INFO:root:ES to Df: 0.91 time elapsed\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.119s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:1.132s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.971s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.146s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.212s]\n",
      "INFO:root:ES to Df: 3.25 time elapsed\n"
     ]
    }
   ],
   "source": [
    "q_one = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=10, tier=\"censhare\", host='localhost', port=9200)\n",
    "q_two = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=10, tier=\"pic\", host='localhost', port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the feature enriched dataset for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:3858 items in the first and second queue\n",
      "INFO:root:10940 items in the second queue only\n",
      "INFO:root:14798 items in the whole dataset\n",
      "INFO:root:Create dataset: 22.15 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X, y, maxlen = create_dataset_train(q_one, q_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scale: 67.88 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_scaled, y_scaled = scale(X, y, start_date=\"2020-11-03\",end_date=\"2020-11-07\", \n",
    "                           epochs='50', steps='720', s_rate='20', model_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad Split: 0.41 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pad_split(X_scaled, y_scaled, maxlen, test_size=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downsample: 0.13 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = downsample(X_train, X_test, y_train, y_test, rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14796, 1440, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/370 [==============================] - 432s 1s/step - loss: 0.0565 - mae: 0.5349 - val_loss: 0.0530 - val_mae: 0.4916\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n_steps = X_train_sampled.shape[1]  # number of steps\n",
    "n_features = X_train_sampled.shape[2]  # number of features\n",
    "\n",
    "# Create model layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(n_steps, n_features)))  # Masking Layer for padding\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(1, input_dim=20))  # Dense Layer to generate 1Dimensional Outputs\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define CallBacks\n",
    "early_stop = EarlyStopping(monitor='mae', mode='min', patience=10)\n",
    "mcp_save = ModelCheckpoint(f'models/mcp_lstm_2epochs_20neurons_10000items_10srate.h5', \n",
    "                            save_best_only=True, monitor='mae', mode='min')\n",
    "\n",
    "# Start training\n",
    "model.fit(X_train_sampled, y_train_sampled, epochs=1, validation_split=0.2,\n",
    "          callbacks=[early_stop, mcp_save])\n",
    "\n",
    "# Save the model in models directory\n",
    "model.save(f'models/lstm_lstm_2epochs_20neurons_10000items_10srate.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Length Test Sample Rate 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.098s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.087s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.082s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.108s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.215s]\n",
      "INFO:root:ES to Df: 1.58 time elapsed\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.106s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.664s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.657s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.149s]\n",
      "INFO:elasticsearch:POST http://localhost:9200/queues/_search [status:200 request:0.214s]\n",
      "INFO:root:ES to Df: 2.37 time elapsed\n"
     ]
    }
   ],
   "source": [
    "q_one = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=20, tier=\"censhare\", host='localhost', port=9200)\n",
    "q_two = es_to_df(start_date=\"2020-06-09\",end_date=\"2020-06-13\", s_rate=20, tier=\"pic\", host='localhost', port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the feature enriched dataset for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:3215 items in the first and second queue\n",
      "INFO:root:11177 items in the second queue only\n",
      "INFO:root:14392 items in the whole dataset\n",
      "INFO:root:Create dataset: 20.00 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X, y, maxlen = create_dataset_train(q_one, q_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scale: 66.02 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_scaled, y_scaled = scale(X, y, start_date=\"2020-11-03\",end_date=\"2020-11-07\", \n",
    "                           epochs='50', steps='720', s_rate='20', model_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pad Split: 0.26 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pad_split(X_scaled, y_scaled, maxlen, test_size=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downsample: 0.07 time elapsed\n"
     ]
    }
   ],
   "source": [
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = downsample(X_train, X_test, y_train, y_test, rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14390, 720, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 205s 568ms/step - loss: 0.0597 - mae: 0.5537 - val_loss: 0.0486 - val_mae: 0.4609\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "n_steps = X_train_sampled.shape[1]  # number of steps\n",
    "n_features = X_train_sampled.shape[2]  # number of features\n",
    "\n",
    "# Create model layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(n_steps, n_features)))  # Masking Layer for padding\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(1, input_dim=20))  # Dense Layer to generate 1Dimensional Outputs\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define CallBacks\n",
    "early_stop = EarlyStopping(monitor='mae', mode='min', patience=10)\n",
    "mcp_save = ModelCheckpoint(f'models/mcp_lstm_2epochs_20neurons_10000items_20srate.h5', \n",
    "                            save_best_only=True, monitor='mae', mode='min')\n",
    "\n",
    "# Start training\n",
    "model.fit(X_train_sampled, y_train_sampled, epochs=1, validation_split=0.2,\n",
    "          callbacks=[early_stop, mcp_save])\n",
    "\n",
    "# Save the model in models directory\n",
    "model.save(f'models/lstm_lstm_2epochs_20neurons_10000items_20srate.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
